\documentclass{article}
\usepackage{simplemargins}

%\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}


\title{A probabilistic approach to k-mer counting}
\author{QP, Jason,  Rose, Adina, CTB}
\begin{document}
\bibliographystyle{plain}
\maketitle



\section{Abstract}
\subsection{Background:}
K-mer counting has been widely used in many Bioinformatics problems, including 
data preprocessing for de novo assembly, repeat detection, and sequencing 
coverage estimation. However, currently available tools cannot handle 
high-throughout data generated by next generation sequencing technology 
efficiently due to high memory requirements or running time.
\subsection{Results:}
Here we present the khmer software package for fast and memory efficient 
counting of k-mers. Unlike previous methods based on data structures such as 
hash tables, suffix arrays, and trie structures, khmer uses a simple 
probabilistic data structure, the counting Bloom filter, which is similar in 
concept to the Count-Min sketch data structure(cite) and can be considered an 
extension of a Bloom filter(cite) . It is highly scalable and has the ability 
to handle large amounts of data, which is difficult to deal with using other 
methods. We compared the memory usage, disk usage and time usage between 
khmer and other software packages like Tallymer and Jellyfish to show the 
advantage of our method. Additionally, we evaluate the counting accuracy of 
our approach. Finally, we present two applications of khmer in the evaluation 
of sequencing error by k-mer abundance distribution in reads and in filtering 
reads for efficient de Bruijn graph-based assembly.
\subsection{Conclusion:}
The khmer software package is an effective and efficient tool in applications 
involving k-mer counting for next generation sequencing data set analysis, 
despite its predictable overestimation properties. Some analysis to data set 
that is too large to be handled by other available software becomes practical 
using our khmer software.
\section{Background}

A k-mer is a substring of length k in a DNA sequence. The goal of k-mer 
counting is to determine the number of occurances for each k-mer in a dataset 
composed of sequence reads. Efficient k-mer counting plays an important role in 
solving many Bioinformatics problems.\\

One important problem is de novo assembly of a large number of short reads. 
With the development of next generation sequencing technology, many research 
groups can afford to sequence samples of specific species or even metagenomic 
samples containing numerous different species\cite{Metzker2010}. Large amount 
of next generation sequencing short reads are generated and de novo assembly 
is required for these sequence data sets\cite{Miller2010}. Currently, de 
Bruijn graph method is popular for de novo assembly of short reads because 
it avoids the need for an all-by-all comparison of reads that scales 
poorly with the number of reads\cite{Miller2010}. Several popular assemblers 
have been developed based on de Bruijn graph, including 
Velvet\cite{Zerbino2008}, ALLPATHS\cite{Butler2008}, ABySS\cite{Simpson2009} 
and SOAPdenovo\cite{Li2010}. 

Each k-mer in a sequence dataset is represented as a vertex in the de Bruijn 
graph. If two k-mers share (k-1)-mer overlap, the two k-mers are connected. 
Since the k-mer is such a basic cornerstone in de Bruijn graph-based assembly, 
it is necessary to determine the occurrence of k-mers. One example of an 
application is for the detection and removal of sequencing errors. Sequencing 
errors can generate many erroneous k-mers, we can filter out the reads with 
too many unique k-mers prior to assembly. Similarly, we can filter out reads 
with k-mers that occur too frequently for smoothing MDA-abundance reads 
datasets. Pre-assembly filtering of reads to reduce dataset sizes is a 
crucial component of time and memory reduction. Additionally, we can use k-mer 
counting to evaluate genome size and the coverage of sequencing reads. K-mer 
counting can also give important information for predicting regions with 
repetitive elements such as transposons.\cite{Kurtz2008}\\

Current methods for k-mer counting involve the data structures like hash 
tables, suffix arrays, binary trees or tries structures(cite). If the size of 
reads dataset for counting is modest, a simple hash table can be used for 
counting, where the k-mers are the keys and the the corresponding counts 
are the values. However, the disadvantage is evident. If the size of sequence 
dataset is sufficiently large, the efficiency of the counting using a simple 
hash table plummets dramatically. Another k-mer counting tool - Tallymer, 
uses a suffix array data structure\cite{Kurtz2008}. It is more efficient 
than simple hash table in general. However, one drawback is that the memory 
requirement is linear to the number of unique k-mers. So this method is not 
so scalable. If the size of a dataset is too large, the cost in time and 
memory is intolerable. For example, in a soil metagenomics project, the size 
of the reads data set generated from one soil sample have already exceeded 
400GB, with only a limited coverage. (to cite GPGC) Similarly-sized datasets 
are too difficult for Tallymer to handle. Jellyfish\cite{Marcais2011}, another 
k-mer counting tool, uses a hash-style data structure but also employs a 
"lock-free" feature to enable parallelism of Jellyfish. However, it still has 
the same problem as Tallymer with a linear increase of memory usage with 
respect to the number of unique k-mers in data set to count.\\

We employ a Bloom counting hash, a simple probabilistic data structure, which 
is similar in concept to Bloom 
filter\cite{DBLP:journals/cacm/Bloom70,DBLP:journals/im/BroderM03,Melsted2011} 
and Count-Min sketch\cite{Cormode2005}, to show a new approach to counting 
k-mers effectively and efficiently. It is highly scalable and can be used to 
count k-mers in large sequence data set with a one-sided tradeoff of counting 
inaccuracy.\\


\section{Methods}
\subsection{Sequence Data}
Two human gut metagenome reads datasets (MH0001 and MH0002) were used from the 
MetaHIT (Metagenomics of the Human Intestinal Tract) project\cite{Qin2010}. 
The MH0001 dataset contains approximately 59 million reads, each 44bp long. 
The MH0002 dataset consists of about 61 million 75bp long reads. Separately, 
we trimmed each FASTA file to remove low quality sequences. 

Five soil metagenomics reads data sets with different size were taken from GPGC 
project for benchmark purpose. Iowa Prairie Table 1\ to cite here

We generated four short read data sets to assess the false positive rate and 
miscount distribution. One is a subset of a real metagenomics data set from the 
MH0001 dataset previously mentioned. The second consists of randomly generated 
reads. The other two contain reads simulated from a random, 1 Mbp long genome. 
One has a substitution error rate of 3\%, and the other one contains no 
errors. The four data sets are generated with different size to include 
similar number of unique k-mers. (k=12)\\

\subsection{Comparing with other k-mer counting programs}
Tallymer is from the genometools package version 1.3.4. It was run with options 
as below: 
For suffixerator subroutine: -dna -pl -tis -suf -lcp ødb

For subroutine tallymer mkindex: -mersize 32  -esa 3

Jellyfish is version 1.1.2 and multithread option is off.

jellyfish count -m 22 -c 2 -C -s 100000000\\
jellyfish merge -o jelly.merged mer\_counts\_*\\
jellyfish stats -o jelly.stats jelly.merged\\

\subsection{A constant-memory probabilistic data structure for k-mer counting}

We implement a simple probabilistic data structure, a Bloom counting hash, for 
fast and memory efficient counting of k-mers. The concept of the Bloom 
counting hash is similar to Bloom filter, with several difference. Firstly, 
the Bloom counting hash consists of one or more hash tables 
rather than a single hash table used in a typical Bloom filter. Secondly, each 
entry in the hash tables is a counter representing the number of k-mers that 
hash to that location, instead of a single bit to 
represent the existence or absense of an item. In our implementation, the hash function 
is to take the modulus of a prime number number close to the size of the 
hash tables. Given a new k-mer, it is hashed to a specific location in each 
hash table using 
the hash function. The counter in that location in each hash table can be 
updated. Like a Bloom filter, different k-mers can be hashed to the same 
location in a hash table, which results in collisions.  The effect of the 
collisions is that the counter in a specific bin may not represent the exact 
count of a specific k-mer, but the maximum possible count of every k-mer that 
can be hashed to that location. If a k-mer has a count of 5 in one hash table, 
that k-mer has an occurrence no more than 5, including 0. On the other hand,
 a k-mer with exactly 5 occurrences may have a counter no less than 5 in a hash 
table. Like the Count-Min sketch, in our implementation, we pick 
the minimum count from all hash table bins for a k-mer to represent its 
occurrence to reduce the effect of collisions. By default, we make each bin 8 
bytes in size for a maximum count of 255, but we also implement an extension 
that can expand the bin size to enable high abundance k-mer counting.

Like Count-Min sketch, this is a probabilistic data structure and the 
counting is not exactly correct. There is a one-sided error, which can  
result in an overestimate, but cannot be smaller 
than actual count. The chosen parameters of the data structure will 
influence the 
accuracy of the count, which can be estimated well like Count-Min 
sketch\cite{Cormode2005}. Such probabilistic properties suit the next 
generation sequencing short reads data sets well; that is, the counts 
are not 
so wrong for next generation sequencing reads data sets because of the 
generally skewed abundance distribution of k-mers in those data sets. We will 
talk more about this in section 4.1.2. Secondly, in many situations the 
accurate count of k-mers is not necessary. For example, when we want to 
eliminate reads with low abundance k-mers, we can tolerate the fact 
that a certain number of reads with low frequency will remain in the resulting 
data set falsely because of the frequency inflation caused by collision. If 
necessary we can do the filtering iteratively so each in step reads with low 
abundance k-mers can be discarded after every iteration. Furthermore, the rate of 
inaccurate counting can be predicted pretty well.  We can 
adjust the parameters of the data structure to make sure the count accuracy 
satisfies our downstream analysis. Secondly, this k-mer counting structure is 
highly scalable. For certain sequence data set, counting error rate is related 
to memory usage. Generally, the more memory we can use, the more accurate the 
counting 
will be. However, no matter how large the data set is, we can predict and 
control the memory usage well with choosing specific parameters. Because of the 
similar characteristics as Bloom filter, given certain parameters like the size 
and number of hash tables to use, the memory usage is constant and independent of 
the length of k-mer and the size of the sequence data set to count. Our method 
will never break an imposed memory bound, unlike some other methods, while 
there is a tradeoff that the miscount rate will be worse.


\subsection{Assessment of counting error}

We want to use the Bloom counting hash to count the occurrence of k-mer in a 
data set. So we define the counting error rate to be the possibility that the 
count is incorrect (off by 1 or more) 

Suppose N unique k-mers have been counted with Z hash tables with size as 
H(Here we assume the hash tables have similar size), the probability that no 
collisions happened in a specific entry in one hash table is 
$(1-1/H)^{N}$, which can be estimated by $e^{-N/H}$. The individual collision 
rate in one hash 
table is $1-e^{-N/H}$. The total collision rate, which is the probability that 
a collision occurred in each entry where a k-mer maps to in all Z hash 
tables, is $(1-e^{-N/H})^{Z}$. In this situation, the counts in 
all Z hash table bins cannot give the true count of a k-mer.


Above we discussed the counting error rate of our counting approach, which 
tells the possibility that a count is incorrect. In some applications like 
abundance filtering, we also want to determine how poor the counts are by 
evaluating the difference between the overestimate and the actual count. 
In the analysis of the Count-Min sketch\cite{Cormode2005}, the offset 
between the incorrect count and actual 
count is related to the total number of k-mers in a dataset and the size of 
each hash table. Specifically, suppose $d$ hash tables with size as $w$ are used in 
counting a dataset with $T$ total k-mers, with probability at least 
$1-e^{-d}$. The offset between an inaccurate count and the actual count is 
smaller than $e*T/w$. This analysis is distribution-independent and, being a worst 
case upper bound, is too conservative. In 
practice, for some data sets, the error estimation is reasonable, but for
other data sets, the data structure we are using can outperform the theoretical 
worst-case bounds by many orders of magnitude. Further study shows that the 
behavior of Count-Min sketch depends on the characteristics of data set such as 
skewness\cite{Rusu2008}. Generally, for low skew data in 
which the k-mers have uniform distribution of frequency, the k-mers can be 
distributed into hash tables evenly with high probability. Thus, the average 
miscount is related to the average abundance of k-mers in the hash tables (T/w). 
In this case, the error estimation is closer to the actual error. Nevertheless, 
for high skew data in which a few k-mers consume a large fraction of the 
total count, generally these high abundance k-mers will be distributed to fewer 
entries in hash tables than low abundance k-mers, which results in fewer 
collision in the counting hash table in general\cite{DBLP:conf/sdm/CormodeM05}. 
This implies that the minimum count in the hash tables will have smaller miscount 
and the 
average miscount will be smaller than that for low skewed data. For 
high skewed dataset, the error estimation should be tighter than the confidence 
bound described above. If the skew of dataset can be determined by other 
methods, a higher bound can be acquired by using the Zipf coefficient. 

According to the discussion above, number of unique k-mers in a data set , 
number of hash tables and size of hash tables will determine the possibility 
that a count is exactly correct, which is defined as counting error rate in 
this paper. Furthermore, the total number of k-mers in a data set, which is 
related to the size of the data set, and hash table size will influence how bad 
the counts are if they are not correct. The hash table number will determine 
the predictability of the error estimation. More used hash tables will increase 
the possibility that the count error is located in a interval, which can be 
predicted from number of total k-mers and hash table size. The k-mer abundance 
distribution also has influence to the degree of miscount. Given a fixed 
number of k-mers and hash table sizes, a skew abundance distribution generally 
has a smaller miscount rate than a uniform abundance distribution.


\subsection{Analysis of memory usage and running time}
Unlike other approaches to count k-mers, memory usage of our bloom count 
approach is independent of the size of the dataset. It only depends on the size(S) 
and number(N) of hash tables to use (i.e. O(SN)).  As discussed above, for a given  
dataset, the size and number of hash tables will determine the accuracy of 
k-mer counting. Thus, the user can control the memory usage based on the 
desired level of accuracy. For time usage, firstly in the first step 
of the counting, which is to consume all k-mers in a data set and store the 
occurrence in hash tables, the time usage depends on the number of total 
k-mers(T), which is related to the size of data set. O(T). The second step of 
the counting is the retrieving of count. So in this step, the time usage 
depends on the number of unique k-mers whose count we want to know.
\subsection{Implementation}

This approach has been implemented in a software named khmer, written in C++ 
with a Python wrapper and freely available under the BSD license. 
http://github.com/ged-lab/khmer. The scripts for the benchmark are also 
included in the khmer repository as well as many handy scripts that can be used 
for many applications.

\section{Results and Discussion}
\subsection{Speed and memory usage on soil metagenomic reads data sets}
To show the time usage and memory usage of the khmer counting approach, we 
counted the number of unique k-mers in 5 soil metagenomic reads data sets with 
different size. 
 
The time usage and memory usage for counting k-mers in 5 soil metagenomic reads 
data sets using khmer and other two programs Tallymer and Jellyfish are shown 
in Figure \ref{cmp_time} and Figure \ref{cmp_memory}.
Figure \ref{cmp_time} shows that the time usage of our khmer approach is 
comparable to the other two programs. From Figure \ref{cmp_memory} we can see 
that the memory usage of both Jellyfish and Tallymer increases linearly with 
data set size, although Jellyfish is more efficient than Tallymer in memory 
usage. This brings a big problem. Nowadays the size of next generation 
sequencing reads data set grows bigger and bigger, the memory usage is becoming 
a heavy burden. For a 5G dataset with 2.7 billion total k-mers, Jellyfish uses 
5G memory and Tallymer cannot handle a 4G dataset with a machine with 24G 
memory. So both tools cannot satisfy the k-mer counting requirement for 
metagenomic data sets. It looks like the memory usage of our khmer approach 
also increases linearly with data set size. This is because we want to keep the 
counting error rate unchanged, like 1\% or 5\%. In fact the memory usage of our 
khmer approach can be adjusted with the tradeoff of counting error rate. We can 
decrease the memory usage by increasing counting error rate as shown in this 
figure. We can also see from the figure that with a decent counting error rate 
like 1\%, the memory usage is still considerably lower compared to other 
programs. 
Another drawback to consider is the disk usage when both Jellyfish and Tallymer 
generate large index files on hard disk.  Figure \ref{cmp_disk} shows that the 
disk usage also increases linearly with the data set size. Here for a dataset 
of 5 gigabytes, the disk usage of both Jellyfish and Tallymer is around 30 
gigabytes. For a metagenomic data set as large as hundreds of gigabytes, the 
huge disk usage is annoying and cannot be neglected. The curve is the size of 
hash tables used in our khmer approach with fixed counting error rate. There is 
an option in our implementation to store the hash tables filled with count on 
hard disk for future use. But the khmer can do everything without doing that

From this comparison, we can conclude that for large metagenomic dataset, other 
k-mer counting programs like Jellyfish and Tallymer fail to do the counting 
successfully because of the high memory usage and disk usage, while our khmer 
counting approach can do the counting, although with the tradeoff of 
inaccuracy. However from the discussion in previous section, the inaccuracy can 
be estimated well and we can evaluate if the counting accuracy suffices the 
requirement of specific analysis with specific limited memory to use.

\subsection{Assessment of false positive rate and miscount distribution}

We have discussed assessment of counting accuracy in section 4.1.2. In 
practice, we are more interested in the performance of the approach when it is 
applied to high diversity dataset. Like metagenomics dataset, a large amount of 
the k-mers in such high diversity dataset is unique. Here we use real 
metagenomics datasets and three simulated high diversity datasets to evaluate 
the counting performance in practice.
 
From Figure \ref{average_offset} it is apparent that with larger hash table, 
the average offset decreases. And the average offset is closely related to the 
number of total k-mers. For example, simulated reads without error has the most 
k-mers out of the four data sets. And the average miscount is the highest. This 
observation proves the conclusion discussed in section 4.1.2, which is, number 
of total k-mers in data set and hash table size will influence how bad the 
counts are if they are not correct.

Figure \ref{average_offset_vs_fpr} shows the relationship between average 
miscount and counting error rate for different test data sets. For fixed 
counting error rate, simulated reads without error has the highest average 
miscount and simulated k-mers has the lowest average miscount. It is because 
they have the highest and lowest number of total k-mers separately. We can have 
more correct counting for real error-prone reads from a genome than for 
randomly generated reads from a genome without error and with a normal 
distribution of k-mer abundance. So here we can conclude that our counting 
approach is more suitable for high diverse data sets, like simulated k-mers and 
real metagenomics data, in which larger proportion of k-mers are low abundance 
even unique due to sequencing errors.


\subsection{Investigation of sequencing error pattern by k-mer abundance 
distribution} 

When dealing with large amount of sequencing data, one annoying concern is the 
sequencing error. Generally the sequencing error occurs randomly. So if k is 
large enough, most of the erroneous k-mers should be unique in the reads 
dataset.  How to detect those sequencing errors and correct or remove them from 
reads is always a challenge. Before doing any data analysis, we should have 
some idea about the sequencing error, like the estimation of sequencing error 
rate, or the distribution of sequencing error rate. Generally quality score of 
sequencing reads can be a good reference to detect errors. According to the 
discussion above, k-mer abundance can be another approach to estimate the 
pattern of sequencing error. This quality-score free method can also be a 
useful approach to evaluate the validity of quality score generated by 
sequencing procedure.

Here we use this method to investigate sequencing error pattern  of an  e.coli 
Illumina reads data set as an example.

As shown in Figure \ref{perc_unique_pos} there are more unique k-mers close to 
the 3' end of reads.  As we have discussed above, sequencing errors can 
generate unique k-mers. Also Figure \ref{perc_high_abun_pos} shows that there 
are more k-mers with high abundance (frequency = 255)  close to the 5' end of 
reads.  

Figure \ref{freq_pos} shows the average frequency of k-mers in different 
position in reads. Because of the relatively higher sequencing error rate, the 
frequency of k-mers close to the two terminals of reads is slightly lower due 
to more unique k-mers. All these results are consistent with the knowledge that 
Illumina reads are error-prone -- especially on the 3' side.

Here we show that using our k-mer counting approach to do the k-mer abundance 
analysis is an effective way to investigate the pattern of sequencing error of 
Illumina reads data.  Knowing such pattern of sequencing error is important for 
choosing proper filtering strategy or choosing appropriate filtering threshold, 
which is otherwise a crucial step in preprocessing data for other data analysis 
like assembly. 

\subsection{Removing reads containing low-abundance k-mers to reduce size of 
data set for efficient assembly}

An important approach to assemble short reads from next generation sequencer is 
the de Bruijn graph, which relies on  k-mer graph. This approach has been 
applied with some success to human microbiome data. Because of technical limit, 
there are sequencing errors in the reads, which bring about erroneous k-mers. 
As discussed in section 5.3, we can detect such erroneous k-mers by detecting 
low frequency k-mers.  So removing or trimming reads containing unique or 
low-abundance k-mers will remove many errors. On the other hand, low-abundance 
k-mers, even some of them are genius,  do not contribute much to the de Bruijn 
graph assembly. So before we do the assembly, we want to filter out the reads 
with low frequency k-mers to decrease memory usage and time usage.

The Bloom counting hash is an efficient and constant-memory data structure for 
filtering reads based on k-mer abundance, and can be used for arbitrary k. One 
approach to k-mer abundance filtering involves removing any read that contains 
even a single low-abundance k-mer. This filtering can be implemented in two 
passes, the first pass for loading k-mers from reads and the second pass for 
filtering the reads. The counting error of the Bloom counting hash manifests as 
a too high count in hash entries with one or more collisions, in which case 
k-mers hashing to that entry may not be correctly flagged as low-abundance. 
High counting error rate therefore manifest as "lenient" filtering, in which 
reads may not be properly removed. However, any read that is removed will be 
correctly removed. To reduce the effect of such counting error rate, we can do 
the filtering iteratively. After each run of filtering, some more reads with 
low-abundance k-mers will be discarded. This graceful degradation in the face 
of large amounts of data is a key property of the Bloom counting hash. 

As an example of this method, we filtered out reads with low abundance k-mers 
from a human gut microbiome metagenomic dataset(MH0001) with more than 42 
million reads. In fact we want any read with any unique k-mer to be discarded. 
We used hash tables with different size to show the influence of hash table 
size. We also showed the effect of iterative filtering to reduce false positive 
rate. To assess the counting error rate, we used Tallymer to get the actual 
accurate count of the k-mers in the dataset. 

From Figure \ref{num_remaining_reads} , we can see that after each run, more 
low-abundance reads were discarded. With larger hash table, the low-abundance 
reads were discarded faster. On the other hand, from Figure 
\ref{perc_remaining_reads}, we can see that after each iteration of filtering, 
the percentage of "bad" reads - reads with unique k-mers  decreased. After four 
iterations, the percentage of "bad" reads was less than 4\%. The result showed 
that with our novel method nearly 40\% of the original reads were discarded by 
removing the low-abundance reads with an acceptable false positive rate (less 
than 4\% after four iterations of filtering). It can reduce the memory and time 
requirement effectively in the following effort of assembly.

\subsection{Scaling the Bloom counting by partitioning k-mer space}
Scaling the Bloom counting hash to extremely large data sets with many unique 
k-mers requires quite a bit of memory: approximately 446 Gb of memory are 
required to achieve a false positive rate of 1\% for $N ? 50x10^9$. It is 
possible to reduce the required memory by dividing k-mer space into multiple 
partitions and counting k-mers separately for each partition. Partitioning 
k-mer space into A partitions results in a linear decrease in the number of 
k-mers under consideration, thus reducing the occupancy by a constant factor A 
and correspondingly reducing the collision rate.

Partitioning k-mer space can be a generalization of the systematic prefix 
filtering approach, where one might first count all k-mers starting with AA, 
then AC, then AG, AT, CA, etc., which is equivalent to partitioning k-mer space 
into 16 equal-sized partitions. These partitions can be calculated 
independently, either across multiple machines or iteratively on a single 
machine, and the results stored for later comparison or analysis.


\section{Conclusions}

K-mer counting has been widely used in many bioinformatics problems, including 
data preprocessing for de novo assembly, repeat detection, sequencing coverage 
estimation. However current available tools cannot handle the high throughout 
data generated by next generation sequencing technology efficiently due to high 
memory requirements or impractically long running time. Here we present the 
khmer software package for fast and memory efficient counting of k-mers. Unlike 
previous methods bases on data structures including hash tables, suffix arrays, 
or trie structures, Khmer uses a simple probabilistic data structure, which is 
similar in concept to Bloom filter and Count-Min sketch data structure. It is 
highly scalable, effective and efficient in applications involving k-mer 
counting to analyze large next generation sequencing dataset, despite with 
certain counting error rate as tradeoff. We compared the memory usage, disk 
usage and time usage between our khmer program and other programs like Tallymer 
and Jellyfish to show the advantage of our method. The counting accuracy was 
also assessed theoretically and was validated using simulated data sets. Our 
counting approach can be used efficiently and effectively for any high diverse 
data set with lots of low-abundance k-mers, like next generation sequencing 
data sets, which are biased towards low-abundance k-mers due to errors. We 
further showed applications of khmer software package in tackling problems like 
detecting sequencing errors in metagenomic reads and removing those erroneous 
reads to reduce data set size through efficient k-mer counting. Our approach 
can also be implemented parallelly or distributably to speed up or handle 
larger data sets with reasonable counting error rate. 


\bibliography{khmer_0814}

\begin{tabular}{ |c | c |c| c| }
  \hline                        
  ~        & size of file(GB) & number of reads & number of unique k-mers  \\
  \hline
        dataset1 & 1.90             & 9,744,399       & 561,178,082             
\\ 
        dataset2 & 2.17             & 19,488,798      & 1,060,354,144           
\\ 
        dataset3 & 3.14             & 29,233,197      & 1,445,923,389           
\\ 
        dataset4 & 4.05             & 38,977,596      & 1,770,589,216           
\\ 
        dataset5 & 5                & 48,721,995      & 2,121,474,237           
\\
  \hline  
\end{tabular}

\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
  ~        & \bigcell{c}{Real metagenomics\\reads} & \bigcell{c}{Totally random 
reads\\with randomly\\ generated k-mers} & \bigcell{c}{Simulated reads\\from 
simulated\\genome with error} & \bigcell{c}{Simulated reads\\from 
simulated\\genome without error}  \\
  \hline
        Size of data set file      & 7.01M    & 3.53M    & 5.92M     & 9.07M 
             \\ 
        Number of total k-mers     & 2917200  & 2250006  & 3757479   & 
5714973          \\ 
        Number of unique k-mers    & 1944996  & 1973430  & 1982403   & 
1991148           \\ 
  \hline  
\end{tabular}

%\graphicspath{./figure/}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_time}}
\caption{Time usage of different khmer counting tools}
\label{cmp_time}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_memory}}
\caption{Memory usage of different k-mer counting tools}
\label{cmp_memory}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_disk}}
\caption{Disk storage usage of different k-mer counting tools}
\label{cmp_disk}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset}}
\caption{average miscount with different hash table size}
\label{average_offset}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset_vs_fpr}}
\caption{relation between average miscount and counting error rate}
\label{average_offset_vs_fpr}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_unique_pos}}
\caption{Percentage of the unique k-mers starting in different position in 
reads}
\label{perc_unique_pos}
\end{figure}
 
\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_high_abun_pos}}
\caption{Percentage of the high abundance k-mers starting in different position 
in reads}
\label{perc_high_abun_pos}
\end{figure}
 
\begin{figure}
\center{\includegraphics[width=5in]{./figure/freq_pos}}
\caption{Average frequency of k-mers starting in different position in reads}
\label{freq_pos}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_remaining_reads}}
\caption{Number of remaining reads after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_remaining_reads}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_remaining_reads}}
\caption{Percentage of incorrect reads in the remaining reads after iterating 
filtering with hash tables with different sizes(1e8 and 1e9) for a human gut 
microbiome metagenomic dataset(MH0001, 42,458,402 reads)}
\label{perc_remaining_reads}
\end{figure}

\end{document}

