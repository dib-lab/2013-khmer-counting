diff --git a/Makefile b/Makefile
old mode 100755
new mode 100644
diff --git a/khmer_0814.tex b/khmer_0814.tex
old mode 100755
new mode 100644
index 425f271..1c0d175
--- a/khmer_0814.tex
+++ b/khmer_0814.tex
@@ -29,8 +29,8 @@ Here we present the khmer software package for fast and memory efficient
 counting of k-mers. Unlike previous methods based on data structures such as 
 hash tables, suffix arrays, and trie structures, khmer uses a simple 
 probabilistic data structure, the counting Bloom filter, which is similar in 
-concept to the Count-Min sketch data structure and can be considered an 
-extension of a Bloom filter. It is highly scalable and has the ability 
+concept to the Count-Min sketch data structure(cite) and can be considered an 
+extension of a Bloom filter(cite) . It is highly scalable and has the ability 
 to handle large amounts of data, which is difficult to deal with using other 
 methods. We compared the memory usage, disk usage and time usage between 
 khmer and other software packages like Tallymer and Jellyfish to show the 
@@ -141,14 +141,14 @@ jellyfish stats -o jelly.stats jelly.merged\\
 
 \subsection{A constant-memory probabilistic data structure for k-mer counting}
 
-We implement a simple probabilistic data structure, a Bloom counting hash, for 
+We present a simple probabilistic data structure, a Bloom counting hash, for 
 fast and memory efficient counting of k-mers. The concept of the Bloom 
 counting hash is similar to Bloom filter, with several difference. Firstly, 
-the Bloom counting hash consists of one or more hash tables 
-rather than a single hash table used in a typical Bloom filter. Secondly, each 
+the Bloom counting hash consists of one or more hash tables of different size 
+instead of one hash table used in a typical Bloom filter. Secondly, each 
 entry in the hash tables is a counter representing the number of k-mers that 
-hash to that location, instead of a single bit to 
-represent the existence or absense of an item. In our implementation, the hash function 
+hash to that location, instead of a single bit in a typical Bloom filter to 
+represent the existence of an item. In our implementation, the hash function 
 is to take the modulus of a prime number number close to the size of the 
 hash tables. Given a new k-mer, it is hashed to a specific location in each 
 hash table using 
@@ -158,45 +158,43 @@ location in a hash table, which results in collisions.  The effect of the
 collisions is that the counter in a specific bin may not represent the exact 
 count of a specific k-mer, but the maximum possible count of every k-mer that 
 can be hashed to that location. If a k-mer has a count of 5 in one hash table, 
-that k-mer has an occurrence no more than 5, including 0. On the other hand,
+which means it has an occurrence no more than 5, including 0. On the other hand,
  a k-mer with exactly 5 occurrences may have a counter no less than 5 in a hash 
-table. Like the Count-Min sketch, in our implementation, we pick 
-the minimum count from all hash table bins for a k-mer to represent its 
-occurrence to reduce the effect of collisions. By default, we make each bin 8 
-bytes in size for a maximum count of 255, but we also implement an extension 
-that can expand the bin size to enable high abundance k-mer counting.
-
-Like Count-Min sketch, this is a probabilistic data structure and the 
-counting is not exactly correct. There is a one-sided error, which can  
-result in an overestimate, but cannot be smaller 
-than actual count. The chosen parameters of the data structure will 
-influence the 
+table. Like the concept of Count-Min sketch, in our implementation, we pick 
+the minimum counter over all the hash tables for a k-mer to represent its 
+occurrence to reduce the effect of collisions. If accurate counting of k-mers 
+is really required, we also implement an extension to counting the high 
+abundance k-mers accurately.
+
+Firstly, like Count-Min sketch, this is a probabilistic data structure and the 
+counting is not exactly correct. There is a one-sided error, which is, any 
+count may be equal to or bigger  than the actual count, but cannot be smaller 
+than actual count because of the collision in the Bloom filter like data 
+structure. The chosen parameters of the data structure will influence the 
 accuracy of the count, which can be estimated well like Count-Min 
-sketch\cite{Cormode2005}. Such probabilistic properties suit the next 
-generation sequencing short reads data sets well; that is, the counts 
-are not 
+sketch\cite{Cormode2005}. In fact such probabilistic properties suit the next 
+generation sequencing short reads data sets well. Firstly, the counts are not 
 so wrong for next generation sequencing reads data sets because of the 
-generally skewed abundance distribution of k-mers in those data sets. We will 
+generally skew abundance distribution of k-mers in those data sets. We will 
 talk more about this in section 4.1.2. Secondly, in many situations the 
-accurate count of k-mers is not necessary. For example, when we want to 
-eliminate reads with low abundance k-mers, we can tolerate the fact 
+accurate count of k-mers is not so required. For example, when we  want to get 
+rid of reads with low abundance k-mers, in practice, we can tolerate the fact 
 that a certain number of reads with low frequency will remain in the resulting 
 data set falsely because of the frequency inflation caused by collision. If 
-necessary we can do the filtering iteratively so each in step reads with low 
-abundance k-mers can be discarded after every iteration. Furthermore, the rate of 
-inaccurate counting can be predicted pretty well.  We can 
-adjust the parameters of the data structure to make sure the count accuracy 
+necessary we can do the filtering iteratively so more and more reads with low 
+abundance k-mers can be thrown after every iteration. Furthermore the rate of 
+inaccurate count and false filtering can be predicted pretty well.  We can 
+twist the parameters of the data structure to make sure the count accuracy 
 satisfies our downstream analysis. Secondly, this k-mer counting structure is 
 highly scalable. For certain sequence data set, counting error rate is related 
-to memory usage. Generally, the more memory we can use, the more accurate the 
-counting 
-will be. However, no matter how large the data set is, we can predict and 
+to memory usage. Generally more memory we can use, more accurate the counting 
+will be. However no matter how large the data set is, we can predict and 
 control the memory usage well with choosing specific parameters. Because of the 
 similar characteristics as Bloom filter, given certain parameters like the size 
-and number of hash tables to use, the memory usage is constant and independent of 
+and number of hash tables to use, the memory usage is constant, independent of 
 the length of k-mer and the size of the sequence data set to count. Our method 
 will never break an imposed memory bound, unlike some other methods, while 
-there is a tradeoff that the miscount rate will be worse.
+there is a tradeoff that the miscount rate will get worse and worse.
 
 
 \subsection{Assessment of counting error}
@@ -208,99 +206,98 @@ count is incorrect (off by 1 or more)
 Suppose N unique k-mers have been counted with Z hash tables with size as 
 H(Here we assume the hash tables have similar size), the probability that no 
 collisions happened in a specific entry in one hash table is 
-$(1-1/H)^{N}$, which can be estimated by $e^{-N/H}$. The individual collision 
-rate in one hash 
+$(1-1/H)^{N}$,which is $e^{-N/H}$. The individual collision rate in one hash 
 table is $1-e^{-N/H}$. The total collision rate, which is the probability that 
-a collision occurred in each entry where a k-mer maps to in all Z hash 
-tables, is $(1-e^{-N/H})^{Z}$. In this situation, the counts in 
-all Z hash table bins cannot give the true count of a k-mer.
-
+collision happened in all the entries where a k-mer is hashed to in all Z hash 
+tables, will be $(1-e^{-N/H})^{Z}$. Only in this situation, all the counters in 
+all Z hash tables cannot give the true count of a k-mer because of the 
+collisions.
 
 Above we discussed the counting error rate of our counting approach, which 
 tells the possibility that a count is incorrect. In some applications like 
-abundance filtering, we also want to determine how poor the counts are by 
-evaluating the difference between the overestimate and the actual count. 
-In the analysis of the Count-Min sketch\cite{Cormode2005}, the offset 
-between the incorrect count and actual 
+abundance filtering we also want to figure out how bad a count is if it is not 
+correct, in other word, what the offset between an inaccurate count and the 
+actual count is. According to the analysis of Count-Min sketch 
+paper,\cite{Cormode2005} the offset between the incorrect count and actual 
 count is related to the total number of k-mers in a dataset and the size of 
-each hash table. Specifically, suppose $d$ hash tables with size as $w$ are used in 
-counting a dataset with $T$ total k-mers, with probability at least 
-$1-e^{-d}$. The offset between an inaccurate count and the actual count is 
-smaller than $e*T/w$. This analysis is distribution-independent and, being a worst 
-case upper bound, is too conservative. In 
-practice, for some data sets, the error estimation is reasonable, but for
+hash table. Specifically, suppose $d$ hash tables with size as $w$ are used in 
+the counting to a dataset with $T$ total k-mers, with probability at least 
+$1-e^{-d}$, the offset between an inaccurate count and the actual count is 
+smaller than $e*T/w$. This is a distribution independent confidence bound and 
+this error estimation is a worst-case upper bound and too conservative. In 
+practice, for some data sets, the error estimation is reasonable, for some 
 other data sets, the data structure we are using can outperform the theoretical 
-worst-case bounds by many orders of magnitude. Further study shows that the 
-behavior of Count-Min sketch depends on the characteristics of data set such as 
-skewness\cite{Rusu2008}. Generally, for low skew data in 
+worst-case bounds in many orders of magnitude. Further study shows that the 
+behavior of Count-Min sketch depends on the characteristics of data set, like 
+if it is skewed or not. \cite{Rusu2008} Generally, for low skew data set in 
 which the k-mers have uniform distribution of frequency, the k-mers can be 
-distributed into hash tables evenly with high probability. Thus, the average 
-miscount is related to the average abundance of k-mers in the hash tables (T/w). 
-In this case, the error estimation is closer to the actual error. Nevertheless, 
-for high skew data in which a few k-mers consume a large fraction of the 
+distributed into hash tables evenly with high probability. So the average 
+miscount is related to the average abundance of k-mers in hash tables ( T/w). 
+In this case, the error estimation is  closer to the actual error. Nevertheless 
+for high skew dataset in which a few k-mers consume a large fraction of the 
 total count, generally these high abundance k-mers will be distributed to fewer 
-entries in hash tables than low abundance k-mers, which results in fewer 
+entries in hash tables than low abundance k-mers and this results in fewer 
 collision in the counting hash table in general\cite{DBLP:conf/sdm/CormodeM05}. 
-This implies that the minimum count in the hash tables will have smaller miscount 
-and the 
-average miscount will be smaller than that for low skewed data. For 
+So the minimum count in the hash tables will have smaller miscount and the 
+average miscount will be smaller than that for low skewed data. This means for 
 high skewed dataset, the error estimation should be tighter than the confidence 
-bound described above. If the skew of dataset can be determined by other 
-methods, a higher bound can be acquired by using the Zipf coefficient. 
+bound described above. If the skew of data set can be determined by other 
+methods, a higher bound can be acquired according to the Zipf coefficient. 
 
 According to the discussion above, number of unique k-mers in a data set , 
 number of hash tables and size of hash tables will determine the possibility 
 that a count is exactly correct, which is defined as counting error rate in 
-this paper. Furthermore, the total number of k-mers in a data set, which is 
+this paper. Furthermore, the number of total k-mers in a data set, which is 
 related to the size of the data set, and hash table size will influence how bad 
 the counts are if they are not correct. The hash table number will determine 
 the predictability of the error estimation. More used hash tables will increase 
 the possibility that the count error is located in a interval, which can be 
 predicted from number of total k-mers and hash table size. The k-mer abundance 
-distribution also has influence to the degree of miscount. Given a fixed 
-number of k-mers and hash table sizes, a skew abundance distribution generally 
-has a smaller miscount rate than a uniform abundance distribution.
+distribution also has influence to the degree of miscount. With certain number 
+of total k-mers in a data set and certain hash table size, skew abundance 
+distribution will have smaller miscount generally than even abundance 
+distribution.
 
 
 \subsection{Analysis of memory usage and running time}
-Unlike other approaches to count k-mers, memory usage of our bloom count 
-approach is independent of the size of the dataset. It only depends on the size(S) 
-and number(N) of hash tables to use (i.e. O(SN)).  As discussed above, for a given  
+Not like many other approaches to count k-mers, memory usage of our bloom count 
+approach is independent of the size of dataset. It only depends on the size(S) 
+and number(N) of hash tables to use. O(S)*O(N) As discussed above, with certain 
 dataset, the size and number of hash tables will determine the accuracy of 
-k-mer counting. Thus, the user can control the memory usage based on the 
-desired level of accuracy. The time usage for the first step of k-mer counting 
-, to consume the k-mers into a counting data structure, depends on 
-the total number of k-mers in the dataset since we must walk through every k-mer 
-in each read. The second step, k-mer retrieval, depends on the number of distinct 
-k-mers in the dataset.
-
+k-mer counting. So the user can control the memory usage totally according to 
+their requirement of accuracy. As to the time usage, firstly in the first step 
+of the counting, which is to consume all k-mers in a data set and store the 
+occurrence in hash tables, the time usage depends on the number of total 
+k-mers(T), which is related to the size of data set. O(T). The second step of 
+the counting is the retrieving of count. So in this step, the time usage 
+depends on the number of unique k-mers whose count we want to know.
 \subsection{Implementation}
 
-Our approach to k-mer counting has been implemented in a software package 
-named khmer, written in C++ 
-with a Python wrapper and freely available under the BSD license 
-at http://github.com/ged-lab/khmer. The scripts for the benchmark are also 
-included in the khmer repository as well as many useful scripts that can be used 
+This approach has been implemented in a software named khmer, written in C++ 
+with a Python wrapper and freely available under the BSD license. 
+http://github.com/ged-lab/khmer. The scripts for the benchmark are also 
+included in the khmer repository as well as many handy scripts that can be used 
 for many applications.
 
 \section{Results and Discussion}
 \subsection{Speed and memory usage on soil metagenomic reads data sets}
 To show the time usage and memory usage of the khmer counting approach, we 
-counted the number of unique k-mers in five soil metagenomic reads data sets with 
-different sizes. 
+counted the number of unique k-mers in 5 soil metagenomic reads data sets with 
+different size. 
  
 The time usage and memory usage for counting k-mers in 5 soil metagenomic reads 
-data sets using khmer in comparison to Tallymer and Jellyfish are shown 
+data sets using khmer and other two programs Tallymer and Jellyfish are shown 
 in Figure \ref{cmp_time} and Figure \ref{cmp_memory}.
 Figure \ref{cmp_time} shows that the time usage of our khmer approach is 
-comparable to the other two programs. From Figure \ref{cmp_memory}, we can see 
+comparable to the other two programs. From Figure \ref{cmp_memory} we can see 
 that the memory usage of both Jellyfish and Tallymer increases linearly with 
-dataset size, although Jellyfish is more efficient than Tallymer in memory 
-usage. For a 5 GB dataset with 2.7 billion total k-mers, Jellyfish uses 
-5 GB memory while Tallymer cannot handle a 4 GB dataset on a machine with 24 GB 
-memory. Thus, both tools cannot satisfy the k-mer counting requirement for 
-metagenomic data sets on computers with a modest amount of memory. In addition, 
-the memory usage of our khmer approach 
+data set size, although Jellyfish is more efficient than Tallymer in memory 
+usage. This brings a big problem. Nowadays the size of next generation 
+sequencing reads data set grows bigger and bigger, the memory usage is becoming 
+a heavy burden. For a 5G dataset with 2.7 billion total k-mers, Jellyfish uses 
+5G memory and Tallymer cannot handle a 4G dataset with a machine with 24G 
+memory. So both tools cannot satisfy the k-mer counting requirement for 
+metagenomic data sets. It looks like the memory usage of our khmer approach 
 also increases linearly with data set size. This is because we want to keep the 
 counting error rate unchanged, like 1\% or 5\%. In fact the memory usage of our 
 khmer approach can be adjusted with the tradeoff of counting error rate. We can 
@@ -310,126 +307,111 @@ like 1\%, the memory usage is still considerably lower compared to other
 programs. 
 Another drawback to consider is the disk usage when both Jellyfish and Tallymer 
 generate large index files on hard disk.  Figure \ref{cmp_disk} shows that the 
-disk usage also increases linearly with the dataset size. For a dataset 
+disk usage also increases linearly with the data set size. Here for a dataset 
 of 5 gigabytes, the disk usage of both Jellyfish and Tallymer is around 30 
-gigabytes. For larger metagenomic datasets containing, disk usage must be 
-economized . 
-The curve is the size of 
-hash tables used in our khmer approach with fixed counting error rate. While 
-the hash tables for k-mer counting can be stored on disk in khmer, this is 
-not a requirement if the data structure can be held in memory for downstream 
-analysis.
-
-From this comparison, we can conclude that for large metagenomic datasets, 
-other 
+gigabytes. For a metagenomic data set as large as hundreds of gigabytes, the 
+huge disk usage is annoying and cannot be neglected. The curve is the size of 
+hash tables used in our khmer approach with fixed counting error rate. There is 
+an option in our implementation to store the hash tables filled with count on 
+hard disk for future use. But the khmer can do everything without doing that
+
+From this comparison, we can conclude that for large metagenomic dataset, other 
 k-mer counting programs like Jellyfish and Tallymer fail to do the counting 
-successfully because of the high memory and disk usage while our khmer 
+successfully because of the high memory usage and disk usage, while our khmer 
 counting approach can do the counting, although with the tradeoff of 
 inaccuracy. However from the discussion in previous section, the inaccuracy can 
-be estimated well, and we can evaluate if the counting accuracy suffices the 
-requirement of a specific Bioinformatic analysis.
+be estimated well and we can evaluate if the counting accuracy suffices the 
+requirement of specific analysis with specific limited memory to use.
 
 \subsection{Assessment of false positive rate and miscount distribution}
 
 We have discussed assessment of counting accuracy in section 4.1.2. In 
 practice, we are more interested in the performance of the approach when it is 
-applied to high diversity datasets. Like metagenomics dataset, 
-a large number of 
-the k-mers in such high diversity datasets is unique. Here we use real 
+applied to high diversity dataset. Like metagenomics dataset, a large amount of 
+the k-mers in such high diversity dataset is unique. Here we use real 
 metagenomics datasets and three simulated high diversity datasets to evaluate 
 the counting performance in practice.
  
 From Figure \ref{average_offset} it is apparent that with larger hash table, 
-the average offset decreases. The average offset is closely related to the 
+the average offset decreases. And the average offset is closely related to the 
 number of total k-mers. For example, simulated reads without error has the most 
-k-mers and highest average miscount out of the four data sets. This 
-observation proves the conclusion discussed in section 4.1.2, which is that 
-the number 
-of total k-mers in the dataset and hash table sizes will influence how bad the 
-counts.
+k-mers out of the four data sets. And the average miscount is the highest. This 
+observation proves the conclusion discussed in section 4.1.2, which is, number 
+of total k-mers in data set and hash table size will influence how bad the 
+counts are if they are not correct.
 
 Figure \ref{average_offset_vs_fpr} shows the relationship between average 
-miscount and counting error rate for different test data sets. For a fixed 
+miscount and counting error rate for different test data sets. For fixed 
 counting error rate, simulated reads without error has the highest average 
-miscount and simulated k-mers has the lowest average miscount. This is 
-because 
-they have the highest and lowest number of total k-mers, respectively. We can 
-have 
+miscount and simulated k-mers has the lowest average miscount. It is because 
+they have the highest and lowest number of total k-mers separately. We can have 
 more correct counting for real error-prone reads from a genome than for 
 randomly generated reads from a genome without error and with a normal 
-distribution of k-mer abundance. Thus, we can conclude that our counting 
+distribution of k-mer abundance. So here we can conclude that our counting 
 approach is more suitable for high diverse data sets, like simulated k-mers and 
 real metagenomics data, in which larger proportion of k-mers are low abundance 
-or unique due to sequencing errors.
+even unique due to sequencing errors.
 
 
 \subsection{Investigation of sequencing error pattern by k-mer abundance 
-distribution}
+distribution}Ê
 
-When dealing with large amount of sequencing data, one pressing issue is  
-sequencing errors. Generally, the sequencing error occurs randomly. If k is 
+When dealing with large amount of sequencing data, one annoying concern is the 
+sequencing error. Generally the sequencing error occurs randomly. So if k is 
 large enough, most of the erroneous k-mers should be unique in the reads 
-dataset. Detecting those sequencing errors to correct or remove them from 
-reads can be challenging with large datasets. Before doing any data analysis, 
-we should have 
-some idea about the sequencing error such as the estimation of the sequencing 
-error 
-rate, or error distribution. Generally, the quality score of 
+dataset.  How to detect those sequencing errors and correct or remove them from 
+reads is always a challenge. Before doing any data analysis, we should have 
+some idea about the sequencing error, like the estimation of sequencing error 
+rate, or the distribution of sequencing error rate. Generally quality score of 
 sequencing reads can be a good reference to detect errors. According to the 
 discussion above, k-mer abundance can be another approach to estimate the 
 pattern of sequencing error. This quality-score free method can also be a 
 useful approach to evaluate the validity of quality score generated by 
 sequencing procedure.
 
-Here we use this method to investigate the sequencing error pattern of an 
-E. coli 
-Illumina reads dataset as an example.
+Here we use this method to investigate sequencing error pattern  of an  e.coli 
+Illumina reads data set as an example.
 
 As shown in Figure \ref{perc_unique_pos} there are more unique k-mers close to 
 the 3' end of reads.  As we have discussed above, sequencing errors can 
 generate unique k-mers. Also Figure \ref{perc_high_abun_pos} shows that there 
-are more k-mers with high abundance (frequency = 255) close to the 5' end of 
+are more k-mers with high abundance (frequency = 255)  close to the 5' end of 
 reads.  
 
 Figure \ref{freq_pos} shows the average frequency of k-mers in different 
-position in reads. Because of the higher sequencing error rate, the 
+position in reads. Because of the relatively higher sequencing error rate, the 
 frequency of k-mers close to the two terminals of reads is slightly lower due 
 to more unique k-mers. All these results are consistent with the knowledge that 
 Illumina reads are error-prone -- especially on the 3' side.
 
-Using our k-mer counting approach to do k-mer abundance 
+Here we show that using our k-mer counting approach to do the k-mer abundance 
 analysis is an effective way to investigate the pattern of sequencing error of 
 Illumina reads data.  Knowing such pattern of sequencing error is important for 
-choosing the proper filtering strategy or filtering threshold, 
-which is a crucial step in preprocessing data for sequence data analysis and 
-manipulation 
+choosing proper filtering strategy or choosing appropriate filtering threshold, 
+which is otherwise a crucial step in preprocessing data for other data analysis 
 like assembly. 
 
 \subsection{Removing reads containing low-abundance k-mers to reduce size of 
 data set for efficient assembly}
 
-The most common approach to assembling short reads from next-generation 
-sequencers is 
-based on the de Bruijn (i.e. k-mer) graph. This approach has been 
-applied with some success to human microbiome data. Because of technical 
-limitations, 
-there are sequencing errors in the reads, resulting in erroneous k-mers. 
+An important approach to assemble short reads from next generation sequencer is 
+the de Bruijn graph, which relies on  k-mer graph. This approach has been 
+applied with some success to human microbiome data. Because of technical limit, 
+there are sequencing errors in the reads, which bring about erroneous k-mers. 
 As discussed in section 5.3, we can detect such erroneous k-mers by detecting 
-low frequency k-mers.  Removing or trimming reads containing unique or 
+low frequency k-mers.  So removing or trimming reads containing unique or 
 low-abundance k-mers will remove many errors. On the other hand, low-abundance 
-k-mers, even though some of them are correct, do not contribute much to the 
-assembly. 
-In light of this fact, we filter out reads with low frequency k-mers to 
-decrease time and memory usage.
+k-mers, even some of them are genius,  do not contribute much to the de Bruijn 
+graph assembly. So before we do the assembly, we want to filter out the reads 
+with low frequency k-mers to decrease memory usage and time usage.
 
 The Bloom counting hash is an efficient and constant-memory data structure for 
 filtering reads based on k-mer abundance, and can be used for arbitrary k. One 
 approach to k-mer abundance filtering involves removing any read that contains 
 even a single low-abundance k-mer. This filtering can be implemented in two 
-passes: the first pass for loading k-mers from reads and the second pass for 
+passes, the first pass for loading k-mers from reads and the second pass for 
 filtering the reads. The counting error of the Bloom counting hash manifests as 
-an overestimated count in hash entries with one or more collisions, in which 
-case 
+a too high count in hash entries with one or more collisions, in which case 
 k-mers hashing to that entry may not be correctly flagged as low-abundance. 
 High counting error rate therefore manifest as "lenient" filtering, in which 
 reads may not be properly removed. However, any read that is removed will be 
@@ -443,30 +425,29 @@ from a human gut microbiome metagenomic dataset(MH0001) with more than 42
 million reads. In fact we want any read with any unique k-mer to be discarded. 
 We used hash tables with different size to show the influence of hash table 
 size. We also showed the effect of iterative filtering to reduce false positive 
-rate. To assess the counting error rate, we used Tallymer to obtain the actual 
+rate. To assess the counting error rate, we used Tallymer to get the actual 
 accurate count of the k-mers in the dataset. 
 
-From Figure \ref{num_remaining_reads}, we see that after each run, more 
+From Figure \ref{num_remaining_reads} , we can see that after each run, more 
 low-abundance reads were discarded. With larger hash table, the low-abundance 
 reads were discarded faster. On the other hand, from Figure 
 \ref{perc_remaining_reads}, we can see that after each iteration of filtering, 
-the percentage of "bad" reads - reads with unique k-mers - decreased. After 
-four 
+the percentage of "bad" reads - reads with unique k-mers  decreased. After four 
 iterations, the percentage of "bad" reads was less than 4\%. The result showed 
-that with our method nearly 40\% of the original reads were discarded by 
+that with our novel method nearly 40\% of the original reads were discarded by 
 removing the low-abundance reads with an acceptable false positive rate (less 
 than 4\% after four iterations of filtering). It can reduce the memory and time 
 requirement effectively in the following effort of assembly.
 
 \subsection{Scaling the Bloom counting by partitioning k-mer space}
 Scaling the Bloom counting hash to extremely large data sets with many unique 
-k-mers requires a large amount of memory: approximately 446 GB of memory is 
+k-mers requires quite a bit of memory: approximately 446 Gb of memory are 
 required to achieve a false positive rate of 1\% for $N ? 50x10^9$. It is 
 possible to reduce the required memory by dividing k-mer space into multiple 
 partitions and counting k-mers separately for each partition. Partitioning 
-k-mer space into $N$ partitions results in a linear decrease in the number of 
-k-mers under consideration, thus reducing the occupancy by a constant factor 
-$n$ and correspondingly reducing the collision rate.
+k-mer space into A partitions results in a linear decrease in the number of 
+k-mers under consideration, thus reducing the occupancy by a constant factor A 
+and correspondingly reducing the collision rate.
 
 Partitioning k-mer space can be a generalization of the systematic prefix 
 filtering approach, where one might first count all k-mers starting with AA, 
diff --git a/simplemargins.sty b/simplemargins.sty
old mode 100755
new mode 100644
