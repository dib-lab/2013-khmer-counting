\documentclass{article}
\usepackage{simplemargins}

%\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

% @CTB do we want to talk about k-mer frequency spectrum instead of k-mer
%      counting?
% @CTB be sure to mention Conway & Bromage paper.
% @CTB Do we need to discuss hash functions?
% @CTB point out that implementation is really easy!
% @CTB discuss cloud computing
% @CTB discuss importance of in-memory counting
% @CTB relate to compression, too.
% @CTB discuss tunable error rate.
% @CTB do we want to look at/implement the distinct-elements counter?
% @CTB add Eric McD to this paper?
% @CTB mention that the current implementation works only for k <= 32,
%      but that this is relatively straightforward.
% @CTB reverse complement issues.
% @CTB ref whatsisname paper

\title{A probabilistic approach to k-mer counting}
\author{QP, Jason,  Rose, Adina, CTB}
\begin{document}
\bibliographystyle{plain}
\maketitle

\section{Abstract}

\paragraph{Introduction:}
K-mer counting is widely used for many purposes in sequence analysis,
including data preprocessing for de novo assembly, repeat detection,
and sequencing coverage estimation. However, currently available
tools have high memory requirements when used on short-read sequencing
data.

\paragraph{Results:}
We present the khmer software package for fast and memory efficient
counting of k-mers in sequences. Unlike previous methods based on data
structures such as hash tables, suffix arrays, and trie structures,
khmer relies entirely on a simple probabilistic data structure, a
Count-Min Sketch.  On sparse data sets, this data structure is
considerably more memory efficient than any possible exact data
structure.  The tradeoff is that the use of a Count-Min Sketch
introduces a systematic positive overcount for k-mers.  Here we
analyze the analysis speed, memory usage, and miscount rate of our
Count-Min Sketch implementation for simulated and real sequencing data
sets.  We also compare the performance of khmer to two other commonly
used k-mer counting packages, Tallymer and Jellyfish.  Finally, we
present two applications of khmer to analyzing short-read sequencing
data: first, for doing reference-free error analysis of short-read data
with k-mer abundances, and second, for trimming errors from short reads
to enable more memory-efficient assembly.

\paragraph{Conclusion:}
The Count-Min Sketch, as implemented in khmer, is an effective and
efficient tool for k-mer counting in biological sequences.  In
particular, the miscount behavior of the Count-Min Sketch performs
well on error-prone short-read sequencing data.  We show that the
khmer package can decrease the memory usage of several k-mer-based
applications by 5-20 fold.

\section{Introduction}

% @CTB will want more here!  But for now, OK.

A k-mer is a substring of length k in a DNA sequence. The goal of
k-mer counting is to determine the number of occurances for each k-mer
in a dataset composed of sequence reads. Efficient k-mer counting
plays an important role in solving many bioinformatics problems.\\

One important problem in biology is de novo assembly of a large number
of short reads.  With the development of next generation sequencing
technology, many research groups can afford to sequence samples of
specific species or even metagenomic samples containing numerous
different species\cite{Metzker2010}. Large numbers of next generation
sequencing short reads can be generated by modern sequencers and often
de novo assembly is required for these sequence data
sets\cite{Miller2010}. De Bruijn graph methods are popular for de novo
assembly of short reads because they avoids the need for an all-by-all
comparison of reads that scales poorly with the number of
reads\cite{Miller2010}. Several popular assemblers have been developed
based on de Bruijn graphs, including Velvet\cite{Zerbino2008},
ALLPATHS\cite{Butler2008}, ABySS\cite{Simpson2009} and
SOAPdenovo\cite{Li2010}.
% @CTB how much do we want to motivate with assembly?  Or do we want
% to go more theoretical and talk about deep isomorphism between
% k-mers and foo, as exemplified by assembly?

Each k-mer in a sequence dataset is represented as a vertex in the de
Bruijn graph. If two k-mers share (k-1)-mer overlap, the two k-mers
are connected.  Since the k-mer is such a basic cornerstone in de
Bruijn graph-based assembly, it is necessary to determine the
occurrence of k-mers. One example of an application is for the
detection and removal of sequencing errors. Sequencing errors can
generate many erroneous k-mers, we can filter out the reads with too
many unique k-mers prior to assembly. Similarly, we can filter out
reads with k-mers that occur too frequently for smoothing
MDA-abundance reads datasets. Pre-assembly filtering of reads to
reduce dataset sizes is a crucial component of time and memory
reduction. Additionally, we can use k-mer counting to evaluate genome
size and the coverage of sequencing reads. K-mer counting can also
give important information for predicting regions with repetitive
elements such as transposons.\cite{Kurtz2008}\\

% @CTB if hash tables are so bad why does jellyfish perform well?

Current methods for k-mer counting involve data structures such as
hash tables, suffix arrays, or binary trees and tries (cite).
Jellyfish uses a hashing data structure to efficiently count k-mers
\cite{Marcais2011}. Tallymer uses a suffix array data
structure\cite{Kurtz2008}.
% @CTB discuss memory usage etc here, in theory.

The central challenge for k-mer counting in short-read data sets is
that these data sets are both relatively sparse and contain many
unique k-mers.  The data sets are sparse because for typical values of
k such as k=32, only a small fraction of the total possible number of
k-mers ($4^{32}$) are actually present in the data set.  However,
because k-mers are generated from error-prone short-read sequencing
reads, there are still many distinct k-mers.
% @CTB expand on this error thingy: talk about genome content, diversity,
% error rate, etc.  Reference diginorm paper?

Our motivation for exploring efficient k-mer counting comes from
metagenomic data, where we have encountered data sets that contain
300e12 bases of DNA and over 50 billion distinct k-mers.

Below, we employ a simple probabilistic data structure for k-mer
counting.  This data structure is an implementation of a Count-Min
Sketch, a generalized probabilistic data structure for storing the
frequency distributions of distinct elements \cite{Cormode2005}.  Our
implementation is based on an extension of the Bloom filter, which has
been previously used for k-mer counting and de Bruijn graph storage
and traversal \cite{DBLP:journals/cacm/Bloom70,
  DBLP:journals/im/BroderM03,Melsted2011}. We provide a software
implementation in a freely available package called 'khmer' and show
that we can efficiently and effectively employ khmer for several
common k-mer counting applications.

% @CTB conclusions we want:
% for given FP rate, memory is linear and v. low memory.
% independent of k.
% FP rate is adjustable.
% and we can iterate.

\section{Methods}

\subsection{Sequence Data}

Two human gut metagenome reads datasets (MH0001 and MH0002) were used from the 
MetaHIT (Metagenomics of the Human Intestinal Tract) project\cite{Qin2010}. 
The MH0001 dataset contains approximately 59 million reads, each 44bp long. 
The MH0002 dataset consists of about 61 million 75bp long reads.
We trimmed each FASTA file to remove low quality sequences. 
% @CTB how did we trim?  What script, exactly?
% @ Adina did this for me. I will ask her about this. -QP

Five soil metagenomics reads data sets with different size were taken
from GPGC project for benchmark purpose.
(Iowa Prairie Table 1\ to cite here.)
% @CTB we will need accession numbers.  Adina has them.

We also generated four short read data sets to assess the false
positive rate and miscount distribution. One is a subset of a real
metagenomics data set from the MH0001 dataset previously
mentioned. The second consists of randomly generated reads. The third
and fourth contain reads simulated from a random, 1 Mbp long genome.
One has a substitution error rate of 3\%, and the other one contains
no errors. The four data sets were chosen to contain identical numbers
of unique 22-mers
% @CTB really @QP? k=12? @k=22 -QP

\subsection{Comparing with other k-mer counting programs}

We counted the number of unique k-mers in five soil metagenomic reads
datasets with different sizes using our khmer counting package and
two other k-mer counting packages - Tallymer and Jellyfish - to compare
the performance. To investigate the k-independance of k-mer counting,
we used both k=22 and k=31.

Tallymer is from the genometools package version 1.3.4. It was run
with the following options. For the suffixerator subroutine we used:
{\tt -dna -pl -tis -suf -lcp}.  We varied the {\tt -parts n} option to
create the index with only 1/n of the total data in main memory, which
reduces the memory usage.  We separately used {\tt -parts 4} and {\tt
  -parts 1} to test performance.

For the mkindex subroutine, we used: {\tt -mersize 31} and {\tt -mersize 22}.

Jellyfish is version 1.1.2 and the multithreading option is off.
% @CTB we may need to justify this later ;)

Jellyfish uses a hash table to store the k-mers and the size of the
hash table can be modified by the user.  When the specified hash table
size is not large enough and fills up, jellyfish writes it to the hard
disk and initializes a new hash table to more k-mers.  Here we use a
similar stratage as in \cite{XX} and chose the minimum size of the hash
tables for Jellyfish so that all k-mers were stored in memory.

We ran Jellyfish with the options as below:

{\tt jellyfish count -m 22 -c 2 -C} for k=22.

{\tt jellyfish count -m 31 -c 2 -C} for k=31.

\subsection{Count-Min Sketch implementation}

We implemented the Count-Min Sketch data structure, a simple
probabilistic data structure for counting distinct elements.  Our
implementation uses $N$ independent hash tables, each containing a
prime number of counters $M$.  The hashing function for each hash
table is fixed, and bijectively converts each DNA k-mer up to k=32)
into a 64-bit number to which the modulus of the hash table size is
applied.  This provides $N$ distinct hash functions (see also
\cite{kmer-percolation}).

To increment the count associated with a k-mer, the counter associated
with the hashed k-mer in each of the $N$ hash tables is incremented.
To retrieve the count associated with a k-mer, the minimum count
across all $N$ hash tables is chosen.

In this scheme, collisions are explicitly not handled, so the count
associated with a k-mer may not be accurate. Because collisions only
falsely {\em increment} counts, however, the retrieved count for any
given k-mer is guaranteed to be no less than the correct count.  Thus
the counting error is one-sided.

\subsection{Source code and scripts}

We implemented this approach to k-mer counting in a software package
named khmer, written in C++ with a Python wrapper.  khmer is freely
available under the BSD license at http://github.com/ged-lab/khmer/.

The version of khmer used to generate the results below is available
at @@@.  Scripts specific to this paper are available in the paper
repository at @@@.

% @CTB should Methods be moved to the end? @QP check Adina's Artifacts
% paper, and PLoS One guidelines.

%\subsection{Benchmarking}

% @CTB do we want to describe how we did benchmarking?
% this is addressed above as section "Comparing with other k-mer
% counting programs}" - QP


\section{Results}

%\subsection{kher memory usage is fixed and low}
%@QP I moved this section to "Discussion"

%\subsection{Collision rates are predictable and measurable}
%@QP I moved this section to "Discussion"


%\subsection{False positive rates and miscount rates are predictable}
%@QP I moved this section to "Discussion"

\subsection{khmer can count k-mers efficiently}

The time usage and memory usage for counting k-mers in five soil metagenomic reads 
data sets using khmer in comparison to Tallymer and Jellyfish are shown 
in Figure \ref{cmp_time} and Figure \ref{cmp_memory}.
Figure \ref{cmp_time} shows that the time usage of our khmer approach is 
comparable to Tallymer, but is slower than Jellyfish. From Figure \ref{cmp_memory}, we see 
that the memory usage of both Jellyfish and Tallymer increases linearly with 
dataset size, although Jellyfish is more efficient than Tallymer in memory 
usage for smaller k size. Using option -parts 4 for Tallymer
subroutine suffix can reduce the memory usage. But the second step of
Tallymer counting method - subroutine mkindex  will always use more
memory as dataset to count increases.
For a 5 GB dataset with 2.7 billion total k-mers, Jellyfish uses 
5 GB memory while Tallymer cannot handle a 4 GB dataset on a machine with 24 GB 
memory. Thus, both tools cannot satisfy the k-mer counting requirement for 
metagenomic data sets on computers with a modest amount of memory. In addition, 
the memory usage of our khmer approach 
also increases linearly with data set size. This is because we want to keep the 
counting error rate unchanged, like 1\% or 5\%. In fact the memory usage of our 
khmer approach can be adjusted with the tradeoff of counting error rate. We can 
decrease the memory usage by increasing counting error rate as shown in this 
figure. We can also see from the figure that with a decent counting error rate 
like 1\%, the memory usage is still considerably lower compared to other 
programs.
Another concern to consider is the disk usage when both Jellyfish and Tallymer 
generate large index files on hard disk.  Figure \ref{cmp_disk} shows that the 
disk usage also increases linearly with the dataset size. For a dataset 
of 5 gigabytes, the disk usage of both Jellyfish and Tallymer is around 30 
gigabytes. For larger metagenomic datasets, disk usage must be 
economized . 
The curve for khmer in Figure \ref{cmp_disk} is the size of 
hash tables used in our khmer approach with fixed counting error rate(1\%). While 
the hash tables for k-mer counting can be stored on disk in khmer, this is 
not a requirement if the data structure can be held in memory for downstream 
analysis.

\subsection{The measured miscount rate is low on short-read data}

Like metagenomics dataset, 
a large number of 
the k-mers in such high diversity datasets are unique. Here we use real 
metagenomics datasets and three simulated high diversity datasets to evaluate 
the counting performance in practice.
 
From Figure \ref{average_offset} it is apparent that with larger hash table, 
the average offset decreases. The average offset is closely related to the 
number of distinct k-mers. For example, simulated reads without error has the most 
k-mers and highest average miscount out of the four data sets.

Figure \ref{average_offset_vs_fpr} shows the relationship between average 
miscount and counting error rate for different test data sets. 


\subsection{Sequencing error profiles can be measured with k-mer abundance
profiles in reads}

When dealing with large amount of sequencing data, one pressing issue is  
sequencing errors. Generally, the sequencing error occurs randomly. If k is 
large enough, most of the erroneous k-mers should be unique in the reads 
dataset. Detecting those sequencing errors to correct or remove them from 
reads can be computationally challenging with large datasets. Before doing any data analysis, 
we should have 
some idea about the sequencing error such as the estimation of the sequencing 
error 
rate, or error distribution. Generally, the quality score of 
sequencing reads can be a good reference to detect errors. According to the 
discussion above, k-mer abundance can be another approach to estimate the 
pattern of sequencing error. This quality-score free method can also be a 
useful approach to evaluate the validity of quality score generated by 
sequencing procedure.

Here we use this method to investigate the sequencing error pattern of an 
E. coli 
Illumina reads dataset as an example.

As shown in Figure \ref{perc_unique_pos} there are more unique k-mers close to 
the 3' end of reads.  As we have discussed above, sequencing errors can 
generate unique k-mers. Also Figure \ref{perc_high_abun_pos} shows that there 
are more k-mers with high abundance (frequency = 255) close to the 5' end of 
reads.  

Figure \ref{freq_pos} shows the average frequency of k-mers in different 
position in reads. Because of the higher sequencing error rate, the 
frequency of k-mers close to the two terminals of reads is slightly lower due 
to more unique k-mers. All these results are consistent with the knowledge that 
Illumina reads are error-prone -- especially on the 3' side.

Using our k-mer counting approach to do k-mer abundance 
analysis is an effective way to investigate the pattern of sequencing error of 
Illumina reads data.  Knowing such pattern of sequencing error is important for 
choosing the proper filtering strategy or filtering threshold, 
which is a crucial step in preprocessing data for sequence data analysis and 
manipulation 
like assembly. 

\subsection{khmer can trim errors from reads}

The most common approach to assembling short reads from next-generation 
sequencers is 
based on the de Bruijn (i.e. k-mer) graph. This approach has been 
applied with some success to human microbiome data. Because of technical 
limitations, 
there are sequencing errors in the reads, resulting in erroneous k-mers. 
As discussed in section 5.3, we can detect such erroneous k-mers by detecting 
low frequency k-mers.  Removing or trimming reads containing unique or 
low-abundance k-mers will remove many errors. On the other hand, low-abundance 
k-mers, even though some of them are correct, do not contribute much to the 
assembly. 
In light of this fact, we filter out reads with low frequency k-mers to 
decrease time and memory usage.

Our k-mer counting approach is efficient for 
filtering reads based on k-mer abundance, and can be used for arbitrary k. One 
approach to k-mer abundance filtering involves removing any read that contains 
even a single low-abundance k-mer. This filtering can be implemented in two 
passes: the first pass for loading k-mers from reads and the second pass for 
filtering the reads. The counting error here manifests as 
an overestimated count in hash entries with one or more collisions, in which 
case 
k-mers hashing to that entry may not be correctly flagged as low-abundance. 
High counting error rate therefore manifest as "lenient" filtering, in which 
reads may not be properly removed. However, any read that is trimmed will be 
correctly trimmed. To reduce the effect of such counting error rate, we can do 
the filtering iteratively. After each run of filtering, some more reads with 
low-abundance k-mers will be discarded. This graceful degradation in the face 
of large amounts of data is a key property of our k-mer counting approach. 

As an example of this method, we filtered out reads with low abundance k-mers 
from a human gut microbiome metagenomic dataset(MH0001) with more than 42 
million reads. In fact we want any read with any unique k-mer to be discarded. 
We used hash tables with different size to show the influence of hash table 
size. We also showed the effect of iterative filtering to reduce false positive 
rate. To assess the counting error rate, we used Tallymer to obtain the actual 
accurate count of the k-mers in the dataset. 

From Figure \ref{num_remaining_reads}, we see that after each run, more 
low-abundance reads were discarded. With larger hash table, the low-abundance 
reads were discarded faster. On the other hand, from Figure 
\ref{perc_remaining_reads}, we can see that after each iteration of filtering, 
the percentage of "bad" reads - reads with unique k-mers - decreased. After 
four 
iterations, the percentage of "bad" reads was less than 4\%. The result showed 
that with our method nearly 40\% of the original reads were discarded by 
removing the low-abundance reads with an acceptable false positive rate (less 
than 4\% after four iterations of filtering). It can reduce the memory and time 
requirement effectively in the following effort of assembly.

\section{Discussion}

\subsection{khmer memory usage is fixed and low}
%@QP I moved this section here from "Result"

Unlike other approaches to count k-mers, memory usage of our bloom
count approach is independent of the size of the dataset. It only
depends on the size(S) and number(N) of hash tables to use
(i.e. O(SN)).  As discussed above, for a given dataset, the size and
number of hash tables will determine the accuracy of k-mer
counting. Thus, the user can control the memory usage based on the
desired level of accuracy. The time usage for the first step of k-mer
counting , to consume the k-mers into a counting data structure,
depends on the total number of k-mers in the dataset since we must
walk through every k-mer in each read. The second step, k-mer
retrieval, depends on the number of distinct k-mers in the dataset.

While the memory usage can be fixed at an arbitrary point, this leads to
different false positive rates.

% @CTB do we want to discuss counting of overflows?


\subsection{khmer scales k-mer counting significantly and usefully}

From the performance comparison between khmer and other two k-mer
counting packages, we can conclude that for large metagenomic datasets, 
it becomes impractical to count k-mers using  Jellyfish and Tallymer
because of the high memory and disk usage while our khmer 
counting approach can do the counting in lower memory and disk usage,
although with the tradeoff of 
inaccuracy. For sparse data set like metagenomic sequencing reads, it
is shown that this memory usage is hard to beat.\cite{Pell2012}

We will also discuss in next section that the inaccuracy can 
be estimated well, and we can evaluate if the counting accuracy suffices the 
requirement of a specific bioinformatic analysis.

 As shown in Figure \ref{cmp_memory}, our khmer approach is 
k-independence. Length of k-mer will not influence
the memory usage, while the memory usage of Jellyfish depends on
length of k-mer heavily. In fact Jellyfish only allows counting
k-mers shorter than 32 while our khmer approach does not have a limit
of length of k-mers to count.



% 1. k independent memory usage.
% 2. Conway & Bromage argument (probably just cite Pell paper) that 
%    this memory usage is hard to beat for sparse data sets/useful k.



%\subsection{Collision rates are predictable and measurable}
%@ I moved this section here from "Result" -QP

% @CTB dramatically shorten this, pls -- put in the basic equations,
% and cite everything else.



%\subsection{False positive rates and miscount rates are predictable}
%@QP I moved this section here from "Result"


\subsection{Error rates in k-mer counting are low and predictable}

Like Count-Min sketch, this is a probabilistic data structure and the 
counting is not exactly correct. There is a one-sided error, which can  
result in an overestimate, but cannot be smaller 
than actual count. The chosen parameters of the data structure will 
influence the accuracy of the count, which can be estimated well like Count-Min 
sketch\cite{Cormode2005}.

Suppose N unique k-mers have been counted with Z hash tables with size as 
H(Here we assume the hash tables have similar size), the probability that no 
collisions happened in a specific entry in one hash table is 
$(1-1/H)^{N}$, which can be estimated by $e^{-N/H}$. The individual collision 
rate in one hash 
table is $1-e^{-N/H}$. The total collision rate, which is the probability that 
a collision occurred in each entry where a k-mer maps to in all Z hash 
tables, is $(1-e^{-N/H})^{Z}$. In this situation, the counts in 
all Z hash table bins cannot give the true count of a k-mer.

Above we discussed the counting error rate of our counting approach, which 
tells the possibility that a count is incorrect.
In some applications like 
abundance filtering, not only we wan to know the possibility that a
count is incorrect, which is the counting error rate, we also want to
determine how poor the count is if it is not correct, which is the
difference between the overestimate and the actual count. 

In the analysis of the Count-Min sketch\cite{Cormode2005}, the offset 
between the incorrect count and actual 
count is related to the total number of k-mers in a dataset and the size of 
each hash table. Specifically, suppose $d$ hash tables with size as $w$ are used in 
counting a dataset with $T$ total k-mers, with probability at least 
$1-e^{-d}$. The offset between an inaccurate count and the actual count is 
smaller than $e*T/w$. This analysis is distribution-independent and, being a worst 
case upper bound, is too conservative. In 
practice, for some data sets, the error estimation is reasonable, but for
other data sets, the data structure we are using can outperform the theoretical 
worst-case bounds by many orders of magnitude. Further study shows that the 
behavior of Count-Min sketch depends on the characteristics of data set such as 
skewness\cite{Rusu2008}. Generally, for low skew data in 
which the k-mers have uniform distribution of frequency, the k-mers can be 
distributed into hash tables evenly with high probability. Thus, the average 
miscount is related to the average abundance of k-mers in the hash tables (T/w). 
In this case, the error estimation is closer to the actual error. Nevertheless, 
for high skew data in which a few k-mers consume a large fraction of the 
total count, generally these high abundance k-mers will be distributed to fewer 
entries in hash tables than low abundance k-mers, which results in fewer 
collision in the counting hash table in general\cite{DBLP:conf/sdm/CormodeM05}. 
This implies that the minimum count in the hash tables will have smaller miscount 
and the 
average miscount will be smaller than that for low skewed data. For 
high skewed dataset, the error estimation should be tighter than the confidence 
bound described above. If the skew of data can be determined by other 
methods, a higher bound can be acquired by using the Zipf coefficient. 

According to the discussion above, number of unique k-mers in a data set , 
number of hash tables and size of hash tables will determine the possibility 
that a count is exactly correct, which is defined as counting error rate in 
this paper. Furthermore, the total number of k-mers in a data set, which is 
related to the size of the data set, and hash table size will influence how bad 
the counts are if they are not correct. The hash table number will determine 
the predictability of the error estimation. More used hash tables will increase 
the possibility that the count error is located in a interval, which can be 
predicted from number of total k-mers and hash table size. The k-mer abundance 
distribution also has influence to the degree of miscount. Given a fixed 
number of k-mers and hash table sizes, a skew abundance distribution generally 
has a smaller miscount rate than a uniform abundance distribution.

Such probabilistic properties suit the next generation sequencing
short reads data sets well; that is, the counts are not 
so wrong for next generation sequencing reads data sets because of the 
generally skewed abundance distribution of k-mers in those data sets. 

Figure \ref{average_offset_vs_fpr} fits the properties very well. It
shows the relationship between average 
miscount and counting error rate for different test data sets. For a fixed 
counting error rate, simulated reads without error has the highest average 
miscount and simulated k-mers has the lowest average miscount.
 This is because they have the highest and lowest number of total
 k-mers, respectively. We can have 
more correct counting for real error-prone reads from a genome than for 
randomly generated reads from a genome without error and with a normal 
distribution of k-mer abundance. Thus, we can conclude that our counting 
approach is more suitable for high diverse data sets, like simulated k-mers and 
real metagenomics data, in which larger proportion of k-mers are low abundance 
or unique due to sequencing errors.



% 1. False positive rates are predictable => collision rates.
% 2. Miscount rates are low on ngs data.
% 3. Therefore this is a generally useful approach.

\subsection{khmer applications}

For many applications, the 
accurate count of k-mers is not necessary. For example, when we want to 
eliminate reads with low abundance k-mers, we can tolerate the fact 
that a certain number of reads with low frequency will remain in the resulting 
data set falsely because of the frequency inflation caused by collision. If 
necessary we can do the filtering iteratively so each in step reads with low 
abundance k-mers can be discarded after every iteration. Furthermore,
because the rate of inaccurate counting can be predicted pretty well,  We can 
adjust the parameters of the data structure to make sure the count accuracy 
satisfies our downstream analysis. This k-mer counting structure is also
highly scalable. For certain sequence data set, counting error rate is related 
to memory usage. Generally, the more memory we can use, the more accurate the 
counting 
will be. However, no matter how large the data set is, we can predict and 
control the memory usage well with choosing specific parameters. Given
certain parameters like the size and number of hash tables to use, 
the memory usage is constant and independent of 
the length of k-mer and the size of the sequence data set to count. Our method 
will never break an imposed memory bound, unlike some other methods, while 
there is a tradeoff that the miscount rate will be worse.


% 1. For many applications, infrequent miscounts will not matter.
% 2. One such example is abundance profile within read.
% 3. Another such example is trimming low-abundance k-mers.
% to expand -QP

\subsection{Future}

% 1. Partitioning k-mer space => multiple runs.
% 2. Runtime optimization. 
% to expand - QP

%\subsection{Conclusions}

%\subsection{Scaling the Bloom counting by partitioning k-mer space}

Scaling the Bloom counting hash to extremely large data sets with many unique 
k-mers requires a large amount of memory: approximately 446 GB of memory is 
required to achieve a false positive rate of 1\% for $N ? 50x10^9$. It is 
possible to reduce the required memory by dividing k-mer space into multiple 
partitions and counting k-mers separately for each partition. Partitioning 
k-mer space into $N$ partitions results in a linear decrease in the number of 
k-mers under consideration, thus reducing the occupancy by a constant factor 
$n$ and correspondingly reducing the collision rate.

Partitioning k-mer space can be a generalization of the systematic prefix 
filtering approach, where one might first count all k-mers starting with AA, 
then AC, then AG, AT, CA, etc., which is equivalent to partitioning k-mer space 
into 16 equal-sized partitions. These partitions can be calculated 
independently, either across multiple machines or iteratively on a single 
machine, and the results stored for later comparison or analysis.


\section{Conclusions}
% 1. Fairly general solution given the error rates.
% 2. Scales a wide range of applications.
% 3. Tunable mem usage/fp rate.

K-mer counting has been widely used in many bioinformatics problems, including 
data preprocessing for de novo assembly, repeat detection, sequencing coverage 
estimation. Here we present the 
khmer software package for fast and memory efficient counting of k-mers. Unlike 
previous methods bases on data structures like hash tables, suffix arrays, 
or trie structures, Khmer uses a simple probabilistic data structure, which is 
similar in concept to Count-Min sketch. It is 
highly scalable, effective and efficient in analyzing large next
generation sequencing dataset involving k-mer counting, despite with 
certain counting error rate as tradeoff. We compared the memory usage, disk 
usage and time usage between our khmer program and other programs like Tallymer 
and Jellyfish to show the advantage of our method. The counting accuracy was 
also assessed theoretically and was validated using simulated data
sets. Our k-mer counting approach can be used
efficiently and effectively for any high diverse dataset with lots of 
low-abundance k-mers, like next generation sequencing 
data sets, which are biased towards low-abundance k-mers due to errors. We 
further showed applications of khmer software package in tackling problems like 
detecting sequencing errors in metagenomic reads and removing those erroneous 
reads to reduce data set size through efficient k-mer counting. Our approach 
can also be implemented parallelly or distributably to speed up or handle 
larger data sets with reasonable counting error rate. 


\bibliography{khmer-counting}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
   & size of file(GB) & number of reads & number of unique
  k-mers & total number of k-mers \\
  \hline
dataset1 & 1.90 & 9,744,399 & 561,178,082 & 630,207,985
\\
dataset2 & 2.17 & 19,488,798 & 1,060,354,144 & 1,259,079,821
\\ 
dataset3 & 3.14 & 29,233,197 & 1,445,923,389 & 1,771,614,378
\\ 
dataset4 & 4.05 & 38,977,596 & 1,770,589,216 & 2,227,756,662
\\ 
dataset5 & 5.00 & 48,721,995 & 2,121,474,237 & 2,743,130,683
\\
  \hline  
\end{tabular}

\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
  ~        & \bigcell{c}{Real metagenomics\\reads} & \bigcell{c}{Totally random 
reads\\with randomly\\ generated k-mers} & \bigcell{c}{Simulated reads\\from 
simulated\\genome with error} & \bigcell{c}{Simulated reads\\from 
simulated\\genome without error}  \\
  \hline
        Size of data set file      & 7.01M    & 3.53M    & 5.92M     & 9.07M 
             \\ 
        Number of total k-mers     & 2917200  & 2250006  & 3757479   & 
5714973          \\ 
        Number of unique k-mers    & 1944996  & 1973430  & 1982403   & 
1991148           \\ 
  \hline  
\end{tabular}

%\graphicspath{./figure/}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_time_v3}}
\caption{Time usage of different khmer counting tools}
\label{cmp_time}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_memory_v3}}
\caption{Memory usage of different k-mer counting tools}
\label{cmp_memory}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_disk}}
\caption{Disk storage usage of different k-mer counting tools}
\label{cmp_disk}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset}}
\caption{average miscount with different hash table size}
\label{average_offset}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset_vs_fpr}}
\caption{relation between average miscount and counting error rate}
\label{average_offset_vs_fpr}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_unique_pos}}
\caption{Percentage of the unique k-mers starting in different position in 
reads}
\label{perc_unique_pos}
\end{figure}
 
\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_high_abun_pos}}
\caption{Percentage of the high abundance k-mers starting in different position 
in reads}
\label{perc_high_abun_pos}
\end{figure}
 
\begin{figure}
\center{\includegraphics[width=5in]{./figure/freq_pos}}
\caption{Average frequency of k-mers starting in different position in reads}
\label{freq_pos}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_remaining_reads}}
\caption{Number of remaining reads after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_remaining_reads}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_remaining_reads}}
\caption{Percentage of incorrect reads in the remaining reads after iterating 
filtering with hash tables with different sizes(1e8 and 1e9) for a human gut 
microbiome metagenomic dataset(MH0001, 42,458,402 reads)}
\label{perc_remaining_reads}
\end{figure}

\end{document}

