\documentclass{article}
\usepackage{simplemargins}

%\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

% @CTB do we want to talk about k-mer frequency spectrum instead of k-mer
%      counting?
% @CTB be sure to mention Conway & Bromage paper.
% @CTB Do we need to discuss hash functions?
% @CTB point out that implementation is really easy!
% @CTB discuss cloud computing
% @CTB discuss importance of in-memory counting
% @CTB relate to compression, too.
% @CTB discuss tunable error rate.
% @CTB do we want to look at/implement the distinct-elements counter?
% @CTB add Eric McD to this paper?
% @CTB mention that the current implementation works only for k <= 32,
%      but that this is relatively straightforward.
% @CTB reverse complement issues.
% @CTB ref whatsisname paper

\title{A probabilistic approach to k-mer counting}
\author{QP, Jason, Eric McDonald, Rose, Adina, CTB}
\begin{document}
\bibliographystyle{plain}
\maketitle

\section{Abstract}

\paragraph{Introduction:}

K-mer abundance analysis is widely used for many purposes in sequence
analysis, including data preprocessing for de novo assembly, repeat
detection, and sequencing coverage estimation.  Recently, a number of
new k-mer counting libraries have emerged to handle the increasing
amount of data available from sequencing platforms; these libraries
offer various tradeoffs between memory and disk usage.

\paragraph{Results:}

We present the khmer software package for fast and memory efficient
counting of individual k-mer abundances in sequencing data
sets. Unlike previous methods based on data structures such as hash
tables, suffix arrays, and trie structures, khmer relies entirely on a
simple probabilistic data structure, a Count-Min Sketch.  On sparse
data sets, this data structure is considerably more memory efficient
than any exact data structure.  The tradeoff for this memory
efficiency is that the use of a Count-Min Sketch introduces a
systematic positive overcount for k-mers.  Here we analyze the
analysis speed, memory usage, and miscount rate of our Count-Min
Sketch implementation for generating the k-mer frequency distribution
in simulated and real sequencing data sets.  We also compare the
performance of khmer to several other k-mer counting packages,
Tallymer, Jellyfish, and DSK.  Finally, we present two applications of
khmer on short-read sequencing data: first, doing reference-free error
analysis of short-read data with k-mer abundances; and second,
trimming errors from short reads.

\paragraph{Conclusion:}

The Count-Min Sketch, as implemented in khmer, is an effective and
efficient tool for k-mer counting in biological sequences.  In
particular, the miscount behavior of the Count-Min Sketch performs
well on error-prone short-read sequencing data.  We show that the
khmer package offers an efficient set of tradeoffs for certain k-mer
counting applications.  khmer is implemented in C++ wrapped with a
Python interface, and is freely available under the BSD license at
github.com/ged-lab/khmer.

\section{Introduction}

A k-mer is a substring of length k in a DNA sequence, where k is
usually between 4 and the length of a short sequencing read. The goal
of k-mer counting is to determine the number of occurrences for each
k-mer in a dataset composed of many sequence reads. Efficient k-mer
counting plays an important role in many bioinformatics approaches,
including data preprocessing for de novo assembly, repeat detection,
and sequencing coverage estimation

One problem of particular interest in biology is {\em de novo}
assembly of a large number of short reads.  With the continued
development of sequencing technologies, many research groups can
afford to sequence genomes and transcriptomes of single species, or
even mixed samples containing numerous different
species\cite{Metzker2010}. Large numbers of next generation sequencing
short reads can be generated by modern sequencers and often
reference-free approaches, e.g. de novo assembly, are applied for
these sequence data sets\cite{Miller2010}. De Bruijn graph methods are
popular for de novo assembly of short reads because they avoid the
need for an all-by-all comparison of reads, which scales poorly with
the number of reads\cite{Miller2010}. Many popular assemblers have
been developed based on de Bruijn graphs, including
Velvet\cite{Zerbino2008}, ALLPATHS\cite{Butler2008},
ABySS\cite{Simpson2009} and SOAPdenovo\cite{Li2010}.

In a de Bruijn assembly graph, each k-mer in a sequence dataset is
represented as a node in the graph. If two k-mers share (k-1)-mer
overlap, the two k-mers are connected; the graph can then be
traversed to extract likely contiguous sequences.  Since k-mers are the
basis for de Bruijn graph-based assembly, k-mer frequency analysis
is an important way to preprocess data and study graph characteristics.

On particular application of k-mer frequency analysis is the detection
and removal of sequencing errors.  Because sequencing errors generate
many erroneous k-mers with low abundance, we can optimize more
heavyweight computational approaches such as assembly by filtering out
reads with too many low-abundance k-mers prior to assembly. Similarly,
we can estimate coverage and remove redundancy by analyzing k-mer
distributions within reads \cite{diginorm}.  In both cases,
pre-assembly filtering of reads to reduce dataset sizes is a crucial
component of time and memory reduction.

Additionally, we can use k-mer counting to evaluate genome size and
the coverage of sequencing reads, which can help determine parameter
settings and analyze assembly results. K-mer counting can also give
important information for predicting regions with repetitive elements
such as transposons.\cite{Kurtz2008}\\

Current methods for k-mer counting involve data structures such as
hash tables, suffix arrays, or binary trees and tries (cite).
Jellyfish uses a hashing data structure to efficiently count k-mers
\cite{Marcais2011}. Tallymer uses a suffix array data
structure\cite{Kurtz2008}. DSK does XXX.

% @CTB discuss memory usage etc here, in theory.

The central challenge for k-mer counting in short-read shotgun
sequencing data set is that these data sets are both relatively sparse
and contain many unique k-mers.  The data sets are sparse because for
typical values of k such as k=32, only a small fraction of the total
possible number of k-mers ($4^{32}$) are actually present in the data
set.  The high error rate is another challenge to analyzing short-read
shotgun sequencing. For example, Illumina has a 0.1-1\% error rate. As
the total number of reads generated increases, the total number of
errors grows linearly, leading to data sets where the erroneous k-mers
vastly outnumber the true k-mers.  Dealing with these large amount of
k-mers, most of which are erroneous, has become an unavoidable challenging
task \cite{Minoche2011}.

Our motivation for exploring efficient k-mer counting comes from
metagenomic data, where we routinely encounter data sets that contain
300e12 bases of DNA and over 50 billion distinct k-mers
\cite{adina_assembly}.

Below, we employ a simple probabilistic data structure for k-mer
counting.  This data structure is an implementation of a Count-Min
Sketch, a generalized probabilistic data structure for storing the
frequency distributions of distinct elements \cite{Cormode2005}.  Our
implementation is based on an extension of the Bloom filter, which has
been previously used for k-mer counting and de Bruijn graph storage
and traversal \cite{DBLP:journals/cacm/Bloom70,
  DBLP:journals/im/BroderM03,Melsted2011}.
  
This approach is particularly memory efficient, with memory usage that
can significantly outperform exact data structures \cite{pellpnas,
  ConwayBromage}.  However, the use of a probabilistic data structure
introduces counting errors, which have effects that must be analyzed
in the context of specific problems.  Below, we analyze the effects of
this counting error on calculations of the frequency distribution for
sequencing data sets, and in particular on metagenomic data sets.

@CTB must mention more about melsted.

\section{Results}

\subsection{Implementing a CountMin Sketch for k-mers}

The implementation details are similar to those of the Bloom filter in
\cite{pellpnas}, but with the use of 8 bit counters instead of 1 bit
counters.  Briefly, Z hash tables are allocated, each with a different
size of approximately H bytes; the sum of these hash table sizes must
fit within available main memory.  To increment the count for a
particular k-mer, a single hash is computed for the k-mer, and the
modulus of that hash with each hash table's size H gives the location
for each hash table; the associated count in each hash table is then
incremented by 1.  To retrieve the count for a k-mer, the same hash is
computed and the minimum count across all hash tables is computed.
While different in detail from the standard CountMin Sketch
implementation, which uses a single hash table with many hash
functions, the performance details are identical \cite{pellpnas}.

To analyze the false positive rate -- the probability with which a
given k-mer count will be incorrect -- we can look at the hash table
load. Suppose N unique k-mers have been counted using Z hash tables,
each with size H.  The probability that no collisions happened in a
specific entry in one hash table is $(1-1/H)^{N}$, which can be
approximated as $e^{-N/H}$. The individual collision rate in one hash
table is $1-e^{-N/H}$. The total collision rate, which is the
probability that a collision occurred in each entry where a k-mer maps
to in all Z hash tables, is $(1-e^{-N/H})^{Z}$. In this situation, the
counts in all Z hash table bins cannot give the true count of a k-mer.

While the false positive rate can easily be calculated from the hash
table load, the average {\em miscount} depends on the k-mer frequency
distribution.  We analyze this below.

\subsection{khmer can count k-mers efficiently}

We measured the time and memory usage for k-mer counting in five soil
metagenomic read data sets using khmer, Tallymer, and Jellyfish
(Figure \ref{cmp_time} and Figure \ref{cmp_memory}).

Figure \ref{cmp_time} shows that the time usage of our khmer approach
is comparable to Tallymer, but is slower than Jellyfish. From Figure
\ref{cmp_memory}, we see that the memory usage of both Jellyfish and
Tallymer increases linearly with dataset size, although Jellyfish is
more efficient than Tallymer in memory usage for smaller k size. Using
option -parts 4 for Tallymer subroutine suffix reduces the memory
usage. But the second step of Tallymer counting method - subroutine
mkindex -- will always use more memory as the dataset to count
increases.  For a 5 GB dataset with 2.7 billion total k-mers,
Jellyfish uses 5 GB memory; Tallymer exceeds 24 GB of memory usage for
a smaller 4 GB data set.
% @CTB we need to choose data sets with the same k-mer #.
In addition, the memory usage of our khmer approach also increases
linearly with data set size as long as we hold the error rate
constant.  However, the memory usage of khmer varies substantially
with the error rate: we can decrease the memory usage by increasing
counting error rate as shown in Figure \ref{cmp_memory}.  We can also
see from the figure that with a low counting error rate of 1\%, the
memory usage is still considerably lower than other programs.

Another concern is the disk usage: both Jellyfish and Tallymer use
large index files stored on the hard disk.  Figure \ref{cmp_disk}
shows that the disk usage also increases linearly with the dataset
size. For a dataset of 5 GB, the disk usage of both Jellyfish and
Tallymer is around 30 GB.  khmer does not rely on any on-disk storage,
although for practicality the hash tables can be saved for later
reuse; thus the disk usage for khmer in Figure \ref{cmp_disk} is the
same as its memory.

@CTB more discussions about DSK...

\subsection{The measured counting error rate is low on short-read data}

A large number of the k-mers in such high diversity metagenomics
dataset are unique (cite Melsted). Here we use both real and simulated
datasets to evaluate the counting performance in practice.  Our
primary measure for evaluation is the {\em offset}, which is the
difference between the true k-mer frequency and the frequency reported
by khmer.

Figure \ref{average_offset_vs_fpr} shows the relationship between average 
miscount and counting error rate for different test data sets.

Even when the counting error rate is as high as 0.9, which means that
90\% of k-mers will have an incorrect count, the average offset can
still be kept pretty low.

% @CTB can you also show this figure with % miscount?
% @QP this is the average offset(miscount) for all the k-mers with different counting
% numbers. Does the % make sense?
% @CTB what is the coverage of the simulated data sets?
% @QP coverage=3


\subsection{Sequencing error profiles can be measured with k-mer abundance
profiles in reads}

One specific use for khmer is detecting random sequencing errors by
looking at the k-mer spectrum within reads (cite Pevzner spectral
analysis).  Low-abundance k-mers contained in a high-coverage data set
typically represent random sequencing errors, and a variety of read
trimming and error correcting tools use k-mer counting to reduce the
error content of the read data set, independent of quality scores or
reference genomes.  This is one application where the counting error
effect of the CountMin Sketch approach used by khmer is tolerable.

Here we look at the sequencing error pattern of an E. coli Illumina
reads dataset as an example.  

%(@CTB which data set? And I'm going to
%assume it's not trimmed in any way, right? Just all reads? And what's
%the estimated coverage?)
%@QP:     ecoli_ref.fastq  (read length: 100bp) It is not trimmed.
%@    got from diginorm paper dataset: /scratch/titus/diginorm/ecoli_ref.fastq
%@ from diginorm paper:
%@ The E. coli,... data sets were taken from Chitsaz et al. [15], and downloaded from bix.ucsd.edu/projects/singlecell/.


As shown in Figure \ref{perc_unique_pos} there are more unique k-mers
close to the 3' end of reads.  This is almost certainly due to the
increased error rate at the 3' end of reads. For metagenomic data sets
with variable coverage, this approach is merely diagnostic for the
entire data set and cannot necessarily be used to trim reads, because
there may be many low abundance k-mers; however, this is useful
information for deciding where error rates are unacceptably high and
reads should be trimmed.

\subsection{Using khmer to trim errors from reads}

% @CTB make sure we make the effects of false positives clear.

As discussed above, we can detect erroneous k-mers in high coverage
data sets by finding low abundance k-mers.  Removing or trimming reads
containing unique or low-abundance k-mers will remove many errors,
which can help scale de Bruijn graph based assembly methods.
(cite Qin MetaHIT paper, Mackelprang paper, and rumen paper, all of
which used low abundance trimming for metagenomic data).

Our k-mer counting approach is efficient for filtering reads based on
k-mer abundance, and can be used for arbitrary k. One approach to
k-mer abundance filtering involves removing any read that contains
even a single low-abundance k-mer.
% @CTB are you using filter-abund? That trims, not removes.
This filtering can be implemented
in two passes: the first pass for loading k-mers from reads and the
second pass for filtering the reads. The counting error here manifests
as an overestimated count in hash entries with one or more collisions,
in which case k-mers hashing to that entry may not be correctly
flagged as low-abundance.  High counting error rate therefore manifest
as "lenient" filtering, in which reads may not be properly
removed. However, any read that is trimmed will be correctly
trimmed. To reduce the effect of such counting error rate, �k-merwe can do
the filtering iteratively. After each run of filtering, some more
reads with low-abundance k-mers will be discarded. This graceful
degradation in the face of large amounts of data is a key property of
this k-mer counting approach.

As an example of this method, we filtered out reads with low abundance
k-mers from a human gut microbiome metagenomic dataset (MH0001) with
more than 42 million reads. In fact we want any read with any unique
k-mer to be discarded.  We used hash tables with different size to
show the influence of hash table size. We also showed the effect of
iterative filtering to reduce false positive rate. To assess the
counting error rate, we used Tallymer to obtain the actual accurate
count of the k-mers in the dataset.

From Figure \ref{num_remaining_reads}, we see that after each run,
more low-abundance reads were discarded. With larger hash table, the
low-abundance reads were discarded faster. On the other hand, from
Figure \ref{perc_remaining_reads}, we can see that after each
iteration of filtering, the percentage of "bad" reads - reads with
unique k-mers - decreased. After four iterations, the percentage of
"bad" reads was less than 4\%. The result showed that with our method
nearly 40\% of the original reads were discarded by removing the
low-abundance reads with an acceptable false positive rate (less than
4\% after four iterations of filtering). It can reduce the memory and
time requirement effectively in the following effort of assembly.

% @CTB what is the initial false positive rate here? It'd be nice
% to show how iterative filtering can lead to substantial improvements
% in the second round, if enough k-mers are removed in the first round.
% Also, can we use ``number of reads with low-abundance k-mers'' instead
% of number of remaining reads on the y axis?
% @QP Figure 7 But here one of the purpose of figure 6 is that it can show the
% iteration of filtering can reduce the number of remaining reads by discarding
% reads with low-abundance k-mers. Figure 7 can not show this information.

\section{Discussion}

\subsection{khmer memory usage is fixed and low}

Memory usage of our CountMin Sketch approach is fixed, which means
that khmer will never crash due to memory limitations, and all
operations can be performed in main memory without recourse to disk
storage.  However, the memory size chosen must be considered in light
of the false positive rate and miscount acceptable for a given
application.  In practice, we have found that a false positive rate of
between 1\% and 10\% offers acceptable performance for a wide range of
tasks (@CTB make sure this is justified above!).

The memory usage of khmer is also quite low for sparse data sets,
especially since only main memory is used and no disk space is
required beyond that required for the read data sets.  This is no
surprise: the information theoretic comparison shown in
\cite{pellpnas} shows that, for sparse sequencing data sets, khmer can
require considerably less memory than any possible exact information
storage.

As discussed above, for a given dataset, the size and number of hash
tables will determine the accuracy of k-mer counting. Thus, the user
can control the memory usage based on the desired level of
accuracy. The time usage for the first step of k-mer counting , to
consume the k-mers into a counting data structure, depends on the
total amount of data since we must walk through every k-mer in each
read. The second step, k-mer retrieval, is essentially constant for
fixed k and Z.

While the memory usage can be fixed at an arbitrary point, this leads
to different false positive rates.  The false positive rate and the
memory usage can be adjusted as trade-off between each other.  We can
reduce the memory usage dramatically if we can accept higher counting
error rate. However the memory usage must be fixed before the counting
by the parameters we choose. These characteristics are important to
make this approach practical in real k-mer counting task. We can
choose appropriate parameters to make full use of the computational
resource while not causing programs to crash.

In our implementation, to reduce the memory usage as much as possible,
we use x byte to store the count of each k-mer in the data
structure. So the maximum count will be 256.  However khmer also
provides an option to use an STL map data structure to store counts
above 256, with the tradeoff of significantly higher memory usage.

\subsection{khmer scales k-mer counting significantly and usefully}

From the performance comparison between khmer and two other k-mer
counting packages, we can conclude that for large metagenomic
datasets, it becomes impractical to count k-mers using Jellyfish and
Tallymer because of the high memory and disk usage.  (What about DKS?)
However, the khmer counting approach can do the counting in lower
memory and disk usage, although with the tradeoff of inaccuracy. For
sparse data set like metagenomic sequencing reads, this memory usage
is better than any possible exact counting scheme \cite{Pell2012}.

We will also discuss in next section that the inaccuracy can be
estimated well, and we can evaluate if the counting accuracy suffices
the requirement of a specific bioinformatic analysis.

As shown in Figure \ref{cmp_memory}, khmer's memory usage is
independent of k. Length of k-mer will not influence the memory usage,
while the memory usage of Jellyfish depends on length of k-mer
heavily. In fact Jellyfish only allows counting k-mers shorter than 32
while our khmer approach does not have a limit of length of k-mers to
count (@CTB).  In fact previous research \ref{ Conway & Bromage,
  Pell}suggests that this memory usage is hard to beat for sparse data
sets/useful k.


\subsection{Error rates in k-mer counting are low and predictable}

The Count-Min sketch is a probabilistic data structure and the
counting is not exactly correct. There is a one-sided error, which can
result in an overestimate, but cannot be smaller than actual
count. The chosen parameters of the data structure will influence the
accuracy of the count, which can be estimated well\cite{Cormode2005}.

Above we discussed about the counting error rate of our counting
approach, which tells the possibility that a count is incorrect.  In
some applications like abundance filtering, not only we wan to know
the possibility that a count is incorrect, which is the counting error
rate, we also want to determine how poor the count is if it is not
correct, which is the difference between the overestimate and the
actual count.

In the analysis of the Count-Min sketch\cite{Cormode2005}, the offset
between the incorrect count and actual count is related to the total
number of k-mers in a dataset and the size of each hash table. Further
study shows that the behavior of Count-Min sketch depends on the
characteristics of data set such as skewness\cite{Rusu2008}.

Generally, number of unique k-mers in a data set, number of hash
tables and size of hash tables will determine the possibility that a
count is exactly correct, which is defined as counting error rate in
this paper. Furthermore, the total number of k-mers in a data set,
which is related to the size of the data set, and hash table size will
influence how bad the counts are if they are not correct. The hash
table number will determine the predictability of the error
estimation. More used hash tables will increase the possibility that
the count error is located in a interval, which can be predicted from
number of total k-mers and hash table size. The k-mer abundance
distribution also has influence to the degree of miscount. Given a
fixed number of k-mers and hash table sizes, a skew abundance
distribution generally has a smaller miscount rate than a uniform
abundance distribution.

Such probabilistic properties suit the next generation sequencing
short reads data sets well; that is, the counts are not so wrong for
next generation sequencing reads data sets because of the generally
skewed abundance distribution of k-mers in those data sets.

Figure \ref{average_offset_vs_fpr} fits the properties very well. It
shows the relationship between average miscount and counting error
rate for different test data sets. For a fixed counting error rate,
simulated reads without error has the highest average miscount and
simulated k-mers has the lowest average miscount.  This is because
they have the highest and lowest number of total k-mers,
respectively. We can have more correct counting for real error-prone
reads from a genome than for randomly generated reads from a genome
without error and with a normal distribution of k-mer abundance. Thus,
we can conclude that our counting approach is more suitable for high
diverse data sets, like simulated k-mers and real metagenomics data,
in which larger proportion of k-mers are low abundance or unique due
to sequencing errors.

In summary this is a generally useful approach. It can be widely used to
deal with next generation sequencing short reads data sets effectively 
and efficiently.

\subsection{khmer applications}

For many applications, the accurate count of k-mers is not
necessary. For example, when we want to eliminate reads with low
abundance k-mers, we can tolerate the fact that a certain number of
reads with low frequency will remain in the resulting data set falsely
because of the frequency inflation caused by collision. If necessary
we can do the filtering iteratively so each in step reads with low
abundance k-mers can be discarded after every iteration. Another
example is to measure sequencing error profiles with k-mer abundance
profiles within reads. Especially as shown in above section, we can
use the abundance distribution of unique k-mers, since the counting
error from collison will not influence the genuineness of these unique
k-mers. Furthermore, because the rate of inaccurate counting can be
predicted pretty well, We can adjust the parameters of the data
structure to make sure the count accuracy satisfies our downstream
analysis. This k-mer counting structure is also highly scalable. For
certain sequence data set, counting error rate is related to memory
usage. Generally, the more memory we can use, the more accurate the
counting will be. However, no matter how large the data set is, we can
predict and control the memory usage well with choosing specific
parameters. Given certain parameters like the size and number of hash
tables to use, the memory usage is constant and independent of the
length of k-mer and the size of the sequence data set to count. Our
method will never break an imposed memory bound, unlike some other
methods, while there is a tradeoff that the miscount rate will be
worse.

\subsection{Future}


Scaling the Bloom counting hash to extremely large data sets with many
unique k-mers requires a large amount of memory: approximately 446 GB
of memory is required to achieve a false positive rate of 1\% for $N ?
50x10^9$. It is possible to reduce the required memory by dividing
k-mer space into multiple partitions and counting k-mers separately
for each partition. Partitioning k-mer space into $N$ partitions
results in a linear decrease in the number of k-mers under
consideration, thus reducing the occupancy by a constant factor $n$
and correspondingly reducing the collision rate.

Partitioning k-mer space can be a generalization of the systematic
prefix filtering approach, where one might first count all k-mers
starting with AA, then AC, then AG, AT, CA, etc., which is equivalent
to partitioning k-mer space into 16 equal-sized partitions. These
partitions can be calculated independently, either across multiple
machines or iteratively on a single machine, and the results stored
for later comparison or analysis.

Runtime optimization. (Eric's work to summarize)

\section{Conclusions}
% 1. Fairly general solution given the error rates.
% 2. Scales a wide range of applications.
% 3. Tunable mem usage/fp rate.

K-mer counting has been widely used in many bioinformatics problems,
including data preprocessing for de novo assembly, repeat detection,
sequencing coverage estimation. Here we present the khmer software
package for fast and memory efficient counting of k-mers. Unlike
previous methods bases on data structures like hash tables, suffix
arrays, or trie structures, Khmer uses a simple probabilistic data
structure, which is similar in concept to Count-Min sketch. It is
highly scalable, effective and efficient in analyzing large next
generation sequencing dataset involving k-mer counting, despite with
certain counting error rate as tradeoff. We compared the memory usage,
disk usage and time usage between our khmer program and other programs
like Tallymer and Jellyfish to show the advantage of our method. The
counting accuracy was also assessed theoretically and was validated
using simulated data sets. Our k-mer counting approach can be used
efficiently and effectively for any high diverse dataset with lots of
low-abundance k-mers, like next generation sequencing data sets, which
are biased towards low-abundance k-mers due to errors. We further
showed applications of khmer software package in tackling problems
like detecting sequencing errors in metagenomic reads and removing
those erroneous reads to reduce data set size through efficient k-mer
counting. Our approach can also be implemented parallelly or
distributably to speed up or handle larger data sets with reasonable
counting error rate.

\section{Methods}

\subsection{Sequence Data}

Two human gut metagenome reads datasets (MH0001 and MH0002) were used from the 
MetaHIT (Metagenomics of the Human Intestinal Tract) project\cite{Qin2010}. 
The MH0001 dataset contains approximately 59 million reads, each 44bp long. 
The MH0002 dataset consists of about 61 million 75bp long reads.
We trimmed each FASTA file to remove low quality sequences. 
% @CTB how did we trim?  What script, exactly?
% @ Adina did this for me. I will ask her about this. -QP

Five soil metagenomics reads data sets with different size were taken
from GPGC project for benchmark purpose.
(Iowa Prairie Table 1\ to cite here.)
% @CTB we will need accession numbers.  Adina has them.

We also generated four short read data sets to assess the false
positive rate and miscount distribution. One is a subset of a real
metagenomics data set from the MH0001 dataset, above. The second consists of randomly generated reads. The third
and fourth contain reads simulated from a random, 1 Mbp long genome.
The third has a substitution error rate of 3\%, and the fourth contains
no errors. The four data sets were chosen to contain identical numbers
of unique 22-mers.

\subsection{Comparing with other k-mer counting programs}

We counted the number of unique k-mers in five soil metagenomic reads
datasets with different sizes using our khmer counting package and
three other k-mer counting packages - Tallymer, Jellyfish and DSK- to compare
the performance. To investigate the k-independence of k-mer counting,
we used both k=22 and k=31.

Tallymer is from the genometools package version 1.3.4, and was run
with the following options. For the suffixerator subroutine we used:
{\tt -dna -pl -tis -suf -lcp}.  We varied the {\tt -parts n} option to
create the index with only 1/n of the total data in main memory, which
reduces the memory usage.  We separately used {\tt -parts 4} and {\tt
  -parts 1} to test performance.

For the mkindex subroutine, we used: {\tt -mersize 31} and {\tt -mersize 22}.

Jellyfish is version 1.1.2 and the multithreading option is off.


Jellyfish uses a hash table to store the k-mers and the size of the
hash table can be modified by the user.  When the specified hash table
size is not large enough and fills up, jellyfish writes it to the hard
disk and initializes a new hash table to more k-mers.  Here we use a
similar stratage as in \cite{XX} and chose the minimum size of the hash
tables for Jellyfish so that all k-mers were stored in memory.

We ran Jellyfish with the options as below:

{\tt jellyfish count -m 22 -c 2 -C} for k=22.

{\tt jellyfish count -m 31 -c 2 -C} for k=31.

We ran DSK with default parameters.

\subsection{Count-Min Sketch implementation}

We implemented the Count-Min Sketch data structure, a simple
probabilistic data structure for counting distinct elements (@cite).  Our
implementation uses $N$ independent hash tables, each containing a
prime number of counters $M$.  The hashing function for each hash
table is fixed, and bijectively converts each DNA k-mer (for k $<=$ 32)
into a 64-bit number to which the modulus of the hash table size is
applied.  This provides $N$ distinct hash functions (see also
\cite{kmer-percolation}).

To increment the count associated with a k-mer, the counter associated
with the hashed k-mer in each of the $N$ hash tables is incremented.
To retrieve the count associated with a k-mer, the minimum count
across all $N$ hash tables is chosen.

In this scheme, collisions are explicitly not handled, so the count
associated with a k-mer may not be accurate. Because collisions only
falsely {\em increment} counts, however, the retrieved count for any
given k-mer is guaranteed to be no less than the correct count.  Thus
the counting error is one-sided.

\subsection{Source code and scripts}

We implemented this approach to k-mer counting in a software package
named khmer, written in C++ with a Python wrapper.  khmer is freely
available under the BSD license at http://github.com/ged-lab/khmer/.

The version of khmer used to generate the results below is available
at @@@.  Scripts specific to this paper are available in the paper
repository at @@@.






\bibliography{khmer-counting}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
   & size of file(GB) & number of reads & number of unique
  k-mers & total number of k-mers \\
  \hline
dataset1 & 1.90 & 9,744,399 & 561,178,082 & 630,207,985
\\
dataset2 & 2.17 & 19,488,798 & 1,060,354,144 & 1,259,079,821
\\ 
dataset3 & 3.14 & 29,233,197 & 1,445,923,389 & 1,771,614,378
\\ 
dataset4 & 4.05 & 38,977,596 & 1,770,589,216 & 2,227,756,662
\\ 
dataset5 & 5.00 & 48,721,995 & 2,121,474,237 & 2,743,130,683
\\
  \hline  
\end{tabular}

\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
  ~������� & \bigcell{c}{Real metagenomics\\reads} & \bigcell{c}{Totally random 
reads\\with randomly\\ generated k-mers} & \bigcell{c}{Simulated reads\\from 
simulated\\genome with error} & \bigcell{c}{Simulated reads\\from 
simulated\\genome without error}  \\
  \hline
��������Size of data set file      & 7.01M��� & 3.53M����& 5.92M��   & 9.07M 
������������ \\ 
��������Number of total k-mers     & 2917200� & 2250006��& 3757479���& 
5714973��������� \\ 
��������Number of unique k-mers    & 1944996  & 1973430��& 1982403�� & 
1991148���������� \\ 
  \hline  
\end{tabular}

%\graphicspath{./figure/}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_time_v3}}
\caption{Time usage of different khmer counting tools}
\label{cmp_time}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_mem_v3}}
\caption{Memory usage of different k-mer counting tools}
\label{cmp_memory}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/cmp_disk}}
\caption{Disk storage usage of different k-mer counting tools}
\label{cmp_disk}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset_vs_fpr}}
\caption{relation between average miscount and counting error rate, note x axis starts at 0.3}
\label{average_offset_vs_fpr}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_unique_pos}}
\caption{Percentage of the unique k-mers starting in different position in 
reads}
\label{perc_unique_pos}
\end{figure}



\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_remaining_reads}}
\caption{Number of remaining reads after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_remaining_reads}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_bad_reads.pdf}}
\caption{Number of reads with low-abundance k-mers after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_bad_reads}
\end{figure}



\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_remaining_reads}}
\caption{Percentage of incorrect reads in the remaining reads after iterating 
filtering with hash tables with different sizes(1e8 and 1e9) for a human gut 
microbiome metagenomic dataset(MH0001, 42,458,402 reads)}
\label{perc_remaining_reads}
\end{figure}

\end{document}

