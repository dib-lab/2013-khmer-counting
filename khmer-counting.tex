\documentclass{article}
\usepackage{simplemargins}

%\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

% @CTB do we want to talk about k-mer frequency spectrum instead of k-mer
%      counting?
% @CTB mention altmet stuff? ``top X packages''
% @CTB be sure to mention Conway & Bromage paper.
% @CTB Do we need to discuss hash functions?
% @CTB point out that implementation is really easy!
% @CTB discuss cloud computing
% @CTB discuss importance of in-memory counting
% @CTB relate to compression, too.
% @CTB discuss tunable error rate.
% @CTB do we want to look at/implement the distinct-elements counter?
% @CTB add Eric McD to this paper?
% @CTB mention that the current implementation works only for k <= 32,
%      but that this is relatively straightforward.
% @CTB reverse complement issues.
% @CTB ref whatsisname paper

\title{A probabilistic approach to k-mer counting}
\author{QP, Jason, Eric McDonald, Rose, Adina, CTB}
\begin{document}
\bibliographystyle{plain}
\maketitle

\section{Abstract}

\paragraph{Introduction:}

K-mer abundance analysis is widely used for many purposes in sequence
analysis, including data preprocessing for de novo assembly, repeat
detection, and sequencing coverage estimation.  Recently, a number of
new k-mer counting libraries have emerged to handle the increasing
amount of data available from sequencing platforms; these libraries
offer various tradeoffs between memory and disk usage.

\paragraph{Results:}

We present the khmer software package for fast and memory efficient
counting of individual k-mer abundances in sequencing data
sets. Unlike previous methods based on data structures such as hash
tables, suffix arrays, and trie structures, khmer relies entirely on a
simple probabilistic data structure, a Count-Min Sketch.  On sparse
data sets, this data structure is considerably more memory efficient
than any exact data structure.  The tradeoff for this memory
efficiency is that the use of a Count-Min Sketch introduces a
systematic positive overcount for k-mers.  Here we analyze the
analysis speed, memory usage, and miscount rate of our Count-Min
Sketch implementation for generating the k-mer frequency distribution
in simulated and real sequencing data sets.  We also compare the
performance of khmer to several other k-mer counting packages,
Tallymer, Jellyfish, and DSK.  Finally, we present two applications of
khmer on short-read sequencing data: first, doing reference-free error
analysis of short-read data with k-mer abundances; and second,
trimming errors from short reads.

\paragraph{Conclusion:}

The Count-Min Sketch, as implemented in khmer, is an effective and
efficient tool for k-mer counting in biological sequences.  In
particular, the miscount behavior of the Count-Min Sketch performs
well on error-prone short-read sequencing data.  We show that the
khmer package offers an efficient set of tradeoffs for certain k-mer
counting applications.  khmer is implemented in C++ wrapped with a
Python interface, and is freely available under the BSD license at
github.com/ged-lab/khmer.

\section{Introduction}

A k-mer is a substring of length k in a DNA sequence, where k is
usually between 4 and the length of a short sequencing read. The goal
of k-mer counting is to determine the number of occurrences for each
k-mer in a dataset composed of many sequence reads. Efficient k-mer
counting plays an important role in many bioinformatics approaches,
including data preprocessing for de novo assembly, repeat detection,
and sequencing coverage estimation

One particular application of k-mer freqency analysis is the detection
and removal of sequencing errors.  Because sequencing errors generate
many erroneous k-mers with low abundance, we can optimize for more
heavyweight computational approaches such as assembly by removing out
reads with too many low-abundance k-mers prior to assembly. Similarly,
we can estimate coverage and remove redundancy by analyzing k-mer
distributions within reads \cite{diginorm}.  In both cases,
pre-assembly filtering of reads to reduce dataset sizes is a crucial
component of time and memory reduction. Additionally, we can use k-mer
counting to evaluate genome size and the coverage of sequencing reads,
which can help determine parameter settings and analyze assembly
results. K-mer counting can also give important information for
predicting regions with repetitive elements such as
transposons.\cite{Kurtz2008}\\

The central challenge for k-mer counting in short-read shotgun
sequencing data set is that these data sets are both relatively sparse
nand contain many unique k-mers.  The data sets are sparse because for
typical values of k such as k=32, only a small fraction of the total
possible number of k-mers ($4^{32}$) are actually present in the data
set.  The high error rate generates many unique k-mers and is another
challenge to analyzing k-mers in short-read shotgun sequencing. For
example, Illumina has a 0.1-1\% error rate. As the total number of
reads generated increases, the total number of errors grows linearly,
leading to data sets where the erroneous k-mers vastly outnumber the
true k-mers.  Dealing with these large amount of k-mers, most of which
are erroneous, has become an unavoidable and challenging task
\cite{Minoche2011}.

% @CTB QP, can you put in citations for these? :)
A variety of k-mer counting approaches, and standalone software
packages implementing them, have emerged in recent years; this
includes Tallymer\cite{Kurtz2008}, Jellyfish\cite{Marcais2011}, BF-Counter\cite{Melsted2011}, DSK\cite{Rizk2013}, and KMC\cite{Deorowicz2013}.  These
approaches and implementations each offer different algorithmic
tradeoffs and different efficiencies, and enable a non-overlapping set
of functionality.  Tallymer uses a suffix tree to store k-mer counts
in memory and on disk.  Jellyfish stores k-mer counts in in-memory
hash tables, and can make use of disk storage to scale to larger
data sets.  BF-Counter uses a Bloom filter as a prefilter to avoid
counting unique k-mers, and is the first published probabilistic approach
to k-mer counting.  DSK adopts a streaming approach to k-mers that
enables time- and memory-efficient k-mer counting with an explicit
tradeoff between disk and memory usage.  And KMC relies primarily
on fast and inexpensive disk access to count k-mers in very little
memory.

Our motivation for exploring efficient k-mer counting comes from our
work with metagenomic data, where we routinely encounter data sets
that contain 300e12 bases of DNA and over 50 billion distinct k-mers
\cite{adina2013}.  In order to efficiently filter, partition, and
assemble these data, we need to store counts for each of these k-mers
in main memory.  This has dictated our exploration of efficient
in-memory k-mer counting techniques.

Below, we describe an implementation of a simple probabilistic data
structure for k-mer counting.  This data structure is based on a
Count-Min Sketch, a generalized probabilistic data structure for
storing the frequency distributions of distinct elements
\cite{Cormode2005}.  Our implementation is based on an extension of
the Bloom filter, which has been previously used for k-mer counting
and de Bruijn graph storage and traversal.
\cite{Bloom70} \cite{BroderM03} \cite{Melsted2011} \cite{Pell2012}


% @CTB mention altmetirc
  
The CountMin Sktech approach is particularly memory efficient, with
memory usage that can significantly outperform exact data structures
\cite{pellpnas, ConwayBromage}.  However, the use of a probabilistic
data structure introduces counting errors, which have effects that
must be analyzed in the context of specific problems.  Below, we
compare CPU, memory and disk usage of our implementation with that of
Tallymer, Jellyfish, and DSK.  We also analyze the effects of this
counting error on calculations of the frequency distribution for
sequencing data sets, and in particular on metagenomic data sets.

\section{Results}

\subsection{Implementing a CountMin Sketch for k-mers}

The implementation details are similar to those of the Bloom filter in
\cite{Pell2012}, but with the use of 8 bit counters instead of 1 bit
counters.  Briefly, Z hash tables are allocated, each with a different
size of approximately H bytes; the sum of these hash table sizes must
fit within available main memory.  To increment the count for a
particular k-mer, a single hash is computed for the k-mer, and the
modulus of that hash with each hash table's size H gives the location
for each hash table; the associated count in each hash table is then
incremented by 1.  To retrieve the count for a k-mer, the same hash is
computed and the minimum count across all hash tables is computed.
While different in detail from the standard CountMin Sketch
implementation, which uses a single hash table with many hash
functions, the performance details are identical \cite{Pell2012}.

To analyze the false positive rate -- the probability with which a
given k-mer count will be incorrect -- we can look at the hash table
load. Suppose N unique k-mers have been counted using Z hash tables,
each with size H.  The probability that no collisions happened in a
specific entry in one hash table is $(1-1/H)^{N}$, which can be
approximated as $e^{-N/H}$. The individual collision rate in one hash
table is $1-e^{-N/H}$. The total collision rate, which is the
probability that a collision occurred in each entry where a k-mer maps
to in all Z hash tables, is $(1-e^{-N/H})^{Z}$. In this situation, the
counts in all Z hash table bins cannot give the true count of a k-mer.

While the false positive rate can easily be calculated from the hash
table load, the average {\em miscount} depends on the k-mer frequency
distribution.  We analyze this below.

\subsection{khmer can count k-mers efficiently}

We measured the time and memory usage for k-mer counting in five soil
metagenomic read data sets using khmer, Tallymer, and Jellyfish
(Figure \ref{cmp_time} and Figure \ref{cmp_memory}).

Figure \ref{cmp_time} shows that the time usage of our khmer approach
is comparable to Tallymer, but is slower than Jellyfish. From Figure
\ref{cmp_memory}, we see that the memory usage of both Jellyfish and
Tallymer increases linearly with dataset size, although Jellyfish is
more efficient than Tallymer in memory usage for smaller k size. Using
option -parts 4 for Tallymer subroutine suffix reduces the memory
usage. But the second step of Tallymer counting method - subroutine
mkindex -- will always use more memory as the dataset to count
increases.  For a 5 GB dataset with 2.7 billion total k-mers,
Jellyfish uses 5 GB memory; Tallymer exceeds 24 GB of memory usage for
a smaller 4 GB data set.
% @CTB we need to choose data sets with the same k-mer #.
In addition, the memory usage of our khmer approach also increases
linearly with data set size as long as we hold the error rate
constant.  However, the memory usage of khmer varies substantially
with the error rate: we can decrease the memory usage by increasing
counting error rate as shown in Figure \ref{cmp_memory}.  We can also
see from the figure that with a low counting error rate of 1\%, the
memory usage is still considerably lower than other programs.

Another concern is the disk usage: both Jellyfish and Tallymer use
large index files stored on the hard disk.  Figure \ref{cmp_disk}
shows that the disk usage also increases linearly with the dataset
size. For a dataset of 5 GB, the disk usage of both Jellyfish and
Tallymer is around 30 GB.  khmer does not rely on any on-disk storage,
although for practicality the hash tables can be saved for later
reuse; thus the disk usage for khmer in Figure \ref{cmp_disk} is the
same as its memory.

@CTB more discussions about DSK...

\subsection{The measured counting error rate is low on short-read data}

A large number of the k-mers in such high diversity metagenomics
dataset are unique \cite{Melsted2011}. Here we use both real and simulated
datasets to evaluate the counting performance in practice.  Our
primary measure for evaluation is the {\em offset}, which is the
difference between the true k-mer frequency and the frequency reported
by khmer.

Figure \ref{average_offset_vs_fpr} shows the relationship between average 
miscount and counting error rate for different test data sets.

Even when the counting error rate is as high as 0.9, which means that
90\% of k-mers will have an incorrect count, the average offset can
still be kept pretty low.

% @CTB can you also show this figure with % miscount?
% @QP this is the average offset(miscount) for all the k-mers with different counting
% numbers. Does the % make sense?
% @CTB what is the coverage of the simulated data sets?
% @QP coverage=3


\subsection{Sequencing error profiles can be measured with k-mer abundance
profiles in reads}

One specific use for khmer is detecting random sequencing errors by
looking at the k-mer spectrum within reads (cite Pevzner spectral
analysis).  Low-abundance k-mers contained in a high-coverage data set
typically represent random sequencing errors, and a variety of read
trimming and error correcting tools use k-mer counting to reduce the
error content of the read data set, independent of quality scores or
reference genomes.  This is one application where the counting error
effect of the CountMin Sketch approach used by khmer is tolerable.

Here we look at the sequencing error pattern of an E. coli Illumina
reads dataset as an example.  

%(@CTB which data set? And I'm going to
%assume it's not trimmed in any way, right? Just all reads? And what's
%the estimated coverage?)
%@QP:     ecoli_ref.fastq  (read length: 100bp) It is not trimmed.
%@    got from diginorm paper dataset: /scratch/titus/diginorm/ecoli_ref.fastq
%@ from diginorm paper:
%@ The E. coli,... data sets were taken from Chitsaz et al. [15], and downloaded from bix.ucsd.edu/projects/singlecell/.


As shown in Figure \ref{perc_unique_pos} there are more unique k-mers
close to the 3' end of reads.  This is almost certainly due to the
increased error rate at the 3' end of reads. For metagenomic data sets
with variable coverage, this approach is merely diagnostic for the
entire data set and cannot necessarily be used to trim reads, because
there may be many low abundance k-mers; however, this is useful
information for deciding where error rates are unacceptably high and
reads should be trimmed.

\subsection{Using khmer to trim errors from reads}

% @CTB make sure we make the effects of false positives clear.

As discussed above, we can detect erroneous k-mers in high coverage
data sets by finding low abundance k-mers.  Removing or trimming reads
containing unique or low-abundance k-mers will remove many errors,
which can help scale de Bruijn graph based assembly methods.
(cite Qin MetaHIT paper, Mackelprang paper, and rumen paper, all of
which used low abundance trimming for metagenomic data).

Our k-mer counting approach is efficient for filtering reads based on
k-mer abundance, and can be used for arbitrary k. One approach to
k-mer abundance filtering involves removing any read that contains
even a single low-abundance k-mer.
% @CTB are you using filter-abund? That trims, not removes.
This filtering can be implemented
in two passes: the first pass for loading k-mers from reads and the
second pass for filtering the reads. The counting error here manifests
as an overestimated count in hash entries with one or more collisions,
in which case k-mers hashing to that entry may not be correctly
flagged as low-abundance.  High counting error rate therefore manifest
as "lenient" filtering, in which reads may not be properly
removed. However, any read that is trimmed will be correctly
trimmed. To reduce the effect of such counting error rate, we can do
the filtering iteratively. After each run of filtering, some more
reads with low-abundance k-mers will be discarded. This graceful
degradation in the face of large amounts of data is a key property of
this k-mer counting approach.

As an example of this method, we filtered out reads with low abundance
k-mers from a human gut microbiome metagenomic dataset (MH0001) with
more than 42 million reads. In fact we want any read with any unique
k-mer to be discarded.  We used hash tables with different size to
show the influence of hash table size. We also showed the effect of
iterative filtering to reduce false positive rate. To assess the
counting error rate, we used Tallymer to obtain the actual accurate
count of the k-mers in the dataset.

From Figure \ref{num_remaining_reads}, we see that after each run,
more low-abundance reads were discarded. With larger hash table, the
low-abundance reads were discarded faster. On the other hand, from
Figure \ref{perc_remaining_reads}, we can see that after each
iteration of filtering, the percentage of "bad" reads - reads with
unique k-mers - decreased. After four iterations, the percentage of
"bad" reads was less than 4\%. The result showed that with our method
nearly 40\% of the original reads were discarded by removing the
low-abundance reads with an acceptable false positive rate (less than
4\% after four iterations of filtering). It can reduce the memory and
time requirement effectively in the following effort of assembly.

% @CTB what is the initial false positive rate here? It'd be nice
% to show how iterative filtering can lead to substantial improvements
% in the second round, if enough k-mers are removed in the first round.
% Also, can we use ``number of reads with low-abundance k-mers'' instead
% of number of remaining reads on the y axis?
% @QP Figure 7 But here one of the purpose of figure 6 is that it can show the
% iteration of filtering can reduce the number of remaining reads by discarding
% reads with low-abundance k-mers. Figure 7 can not show this information.

\section{Discussion}

\subsection{khmer memory usage is fixed and low}

Memory usage of our CountMin Sketch approach is fixed, which means
that khmer will never crash due to memory limitations, and all
operations can be performed in main memory without recourse to disk
storage.  However, the memory size chosen must be considered in light
of the false positive rate and miscount acceptable for a given
application.  In practice, we have found that a false positive rate of
between 1\% and 10\% offers acceptable performance for a wide range of
tasks (@CTB make sure this is justified above!).

The memory usage of khmer is also quite low for sparse data sets,
especially since only main memory is used and no disk space is
necessary beyond that required for the read data sets.  This is no
surprise: the information theoretic comparison shown in
\cite{pellpnas} shows that, for sparse sequencing data sets, Bloom
filters require considerably less memory than any possible exact
information storage for a wide range of error rates and data set
sparseness.

As discussed above, for a given dataset, the size and number of hash
tables will determine the accuracy of k-mer counting. Thus, the user
can control the memory usage based on the desired level of
accuracy. The time usage for the first step of k-mer counting , to
consume the k-mers into a counting data structure, depends on the
total amount of data since we must walk through every k-mer in each
read. The second step, k-mer retrieval, is essentially constant for
fixed k and Z.

In our implementation, to reduce the memory usage as much as possible,
we use 1 byte to store the count of each k-mer in the data
structure. This means that the maximum count for a k-mer will be 255.
In cases where bigger counts are required, khmer also provides an
option to use an STL map data structure to store counts above 255,
with the tradeoff of significantly higher memory usage.  In the
future, we may extend khmer to counters of arbitrary bitsizes.

\subsection{khmer scales k-mer counting significantly and usefully}

From the performance comparison between khmer and two other k-mer
counting packages, we can conclude that for large metagenomic
datasets, it becomes impractical to count k-mers using Jellyfish and
Tallymer because of the high memory and disk usage.  (What about DSK?
@CTB) However, the khmer counting approach can do the counting in
lower memory and disk usage, although with the tradeoff of
inaccuracy. For sparse data sets like metagenomic sequencing reads,
this memory usage is better than any possible exact counting scheme,
\cite{Pell2012}.  More generally, we show above that for sequencing
data with many low-abundance k-mers, probabilistic approaches that
make use of the 1-biased frequency distribution can be used quite
efficiently.

As shown in Figure \ref{cmp_memory}, khmer's memory usage is
independent of k, while the memory usage of Jellyfish depends on
heavily on the length of k-mers.  In addition, for smaller k-mer sizes
and/or less sparse data sets, we would expect suffix tree approaches
such as those used in Tallymer to outperform khmer and Jellyfish.

\subsection{Error rates in k-mer counting are low and predictable}

The Count-Min sketch is a probabilistic data structure and the
counting is not exactly correct. This generates a one-sided error that
can result in an overestimate of k-mer frequency, but cannot generate
an underestimate. The chosen parameters of the data structure will
influence the accuracy of the count.  While the likelihood of an
inaccurate can easily be estimated based on the hash table load
the miscount size is dependent on details of
the frequency distribution of k-mers \cite{Cormode2005}.

More specifically, in the analysis of the Count-Min
sketch\cite{Cormode2005}, the offset between the incorrect count and
actual count is related to the total number of k-mers in a dataset and
the size of each hash table. Further study has shown that the behavior of
Count-Min sketch depends on the characteristics of data set such as
skewness\cite{Rusu2008}.

Such probabilistic properties suit the next generation sequencing
short reads data sets well; that is, the counts are not so wrong for
next generation sequencing reads data sets because of the highly
skewed abundance distribution of k-mers in those data sets.

% @CTB maybe move this to results, or split it between there and here.
Figure \ref{average_offset_vs_fpr} fits the properties very well. It
shows the relationship between average miscount and counting error
rate for different test data sets. For a fixed counting error rate,
simulated reads without error has the highest average miscount and
simulated k-mers has the lowest average miscount.  This is because
they have the highest and lowest number of total k-mers,
respectively. We can have more correct counting for real error-prone
reads from a genome than for randomly generated reads from a genome
without error and with a normal distribution of k-mer abundance. Thus,
we can conclude that our counting approach is more suitable for high
diverse data sets, like simulated k-mers and real metagenomics data,
in which larger proportion of k-mers are low abundance or unique due
to sequencing errors.

\subsection{Khmer applications}

For many applications, the accurate count of k-mers is not
necessary. For example, when we want to eliminate reads with low
abundance k-mers, we can tolerate the fact that a certain number of
reads with low frequency will remain in the resulting data set falsely
because of the frequency inflation caused by collision. If necessary
we can do the filtering iteratively so each in step reads with low
abundance k-mers can be discarded after every iteration. Another
example is to measure sequencing error profiles with k-mer abundance
profiles within reads. Especially as shown in above section, we can
use the abundance distribution of unique k-mers, since the counting
error from collison will not influence the genuineness of these unique
k-mers. Furthermore, because the rate of inaccurate counting can be
predicted pretty well, We can adjust the parameters of the data
structure to make sure the count accuracy satisfies our downstream
analysis. This k-mer counting structure is also highly scalable. For
certain sequence data set, counting error rate is related to memory
usage. Generally, the more memory we can use, the more accurate the
counting will be. However, no matter how large the data set is, we can
predict and control the memory usage well with choosing specific
parameters. Given certain parameters like the size and number of hash
tables to use, the memory usage is constant and independent of the
length of k-mer and the size of the sequence data set to count. Our
method will never break an imposed memory bound, unlike some other
methods, while there is a tradeoff that the miscount rate will be
worse.

\subsection{Future}

Scaling the Bloom counting hash to extremely large data sets with many
unique k-mers requires a large amount of memory: approximately 446 GB
of memory is required to achieve a false positive rate of 1\% for $N ?
50x10^9$. It is possible to reduce the required memory by dividing
k-mer space into multiple partitions and counting k-mers separately
for each partition. Partitioning k-mer space into $N$ partitions
results in a linear decrease in the number of k-mers under
consideration, thus reducing the occupancy by a constant factor $n$
and correspondingly reducing the collision rate.

Partitioning k-mer space can be a generalization of the systematic
prefix filtering approach, where one might first count all k-mers
starting with AA, then AC, then AG, AT, CA, etc., which is equivalent
to partitioning k-mer space into 16 equal-sized partitions. These
partitions can be calculated independently, either across multiple
machines or iteratively on a single machine, and the results stored
for later comparison or analysis.

Multi-core architectures are becoming more popular nowadays. It is 
promising to try to take advantage of them using parallelization.
However, because k-mer 
counting problem relies on high throughput processing of data, I/O bandwidth 
will be essential constraint beyond a certain point of parallelization.
That said, we still implemented the parallelization in some degree. We
have implemented a prefetch buffer in conjunction with direct input that
can be used by multiple threads. Such parallel computing makes the good
 scaling of the khmer software possible.

In the future, after the basic performance issues is solved, we will be
focused on the growing of the programmer's API. Currently Khmer has been
widely used by many groups because of the versatility of this package. 
It can not only do the routine k-mer counting or k-mer abundance distribution
analysis as other k-mer counting tools, but also has a user-friendly API 
which can be used to do more fancy job and many useful scripts working right 
out of the box. Next We will still work on providing more well-characterized
components that can be integrated into larger pipelines and providing more
well tested use cases and documentation to make it more accessible to people
lack of computing experience.



\section{Conclusions}
% 1. Fairly general solution given the error rates.
% 2. Scales a wide range of applications.
% 3. Tunable mem usage/fp rate.
In summary this is a generally useful approach. It can be widely used to
deal with next generation sequencing short reads data sets effectively 
and efficiently.
K-mer counting has been widely used in many bioinformatics problems,
including data preprocessing for de novo assembly, repeat detection,
sequencing coverage estimation. Here we present the khmer software
package for fast and memory efficient counting of k-mers. Unlike
previous methods bases on data structures like hash tables, suffix
arrays, or trie structures, Khmer uses a simple probabilistic data
structure, which is similar in concept to Count-Min sketch. It is
highly scalable, effective and efficient in analyzing large next
generation sequencing dataset involving k-mer counting, despite with
certain counting error rate as tradeoff. We compared the memory usage,
disk usage and time usage between our khmer program and other programs
like Tallymer and Jellyfish to show the advantage of our method. The
counting accuracy was also assessed theoretically and was validated
using simulated data sets. Our k-mer counting approach can be used
efficiently and effectively for any high diverse dataset with lots of
low-abundance k-mers, like next generation sequencing data sets, which
are biased towards low-abundance k-mers due to errors. We further
showed applications of khmer software package in tackling problems
like detecting sequencing errors in metagenomic reads and removing
those erroneous reads to reduce data set size through efficient k-mer
counting. Our approach can also be implemented parallelly or
distributably to speed up or handle larger data sets with reasonable
counting error rate.

\section{Methods}

\subsection{Sequence Data}

Two human gut metagenome reads datasets (MH0001 and MH0002) were used from the 
MetaHIT (Metagenomics of the Human Intestinal Tract) project\cite{Qin2010}. 
The MH0001 dataset contains approximately 59 million reads, each 44bp long. 
The MH0002 dataset consists of about 61 million 75bp long reads.
We trimmed each FASTA file to remove low quality sequences. 
% @CTB how did we trim?  What script, exactly?
% @ Adina did this for me. I will ask her about this. -QP

Five soil metagenomics reads data sets with different size were taken
from GPGC project for benchmark purpose.
(Iowa Prairie Table 1\ to cite here.)
% @CTB we will need accession numbers.  Adina has them.

We also generated four short read data sets to assess the false
positive rate and miscount distribution. One is a subset of a real
metagenomics data set from the MH0001 dataset, above. The second consists of randomly generated reads. The third
and fourth contain reads simulated from a random, 1 Mbp long genome.
The third has a substitution error rate of 3\%, and the fourth contains
no errors. The four data sets were chosen to contain identical numbers
of unique 22-mers.

\subsection{Comparing with other k-mer counting programs}

We counted the number of unique k-mers in five soil metagenomic reads
datasets with different sizes using our khmer counting package and
three other k-mer counting packages - Tallymer, Jellyfish and DSK- to compare
the performance. To investigate the k-independence of k-mer counting,
we used both k=22 and k=31.

Tallymer is from the genometools package version 1.3.4, and was run
with the following options. For the suffixerator subroutine we used:
{\tt -dna -pl -tis -suf -lcp}.  We varied the {\tt -parts n} option to
create the index with only 1/n of the total data in main memory, which
reduces the memory usage.  We separately used {\tt -parts 4} and {\tt
  -parts 1} to test performance.

For the mkindex subroutine, we used: {\tt -mersize 31} and {\tt -mersize 22}.

Jellyfish is version 1.1.2 and the multithreading option is off.


Jellyfish uses a hash table to store the k-mers and the size of the
hash table can be modified by the user.  When the specified hash table
size is not large enough and fills up, jellyfish writes it to the hard
disk and initializes a new hash table to more k-mers.  Here we use a
similar stratage as in \cite{Melsted2011} and chose the minimum size of the hashf 
tables for Jellyfish so that all k-mers were stored in memory.

We ran Jellyfish with the options as below:

{\tt jellyfish count -m 22 -c 2 -C} for k=22.

{\tt jellyfish count -m 31 -c 2 -C} for k=31.

We ran DSK with default parameters.

\subsection{Count-Min Sketch implementation}

We implemented the Count-Min Sketch data structure, a simple
probabilistic data structure for counting distinct elements (@cite).  Our
implementation uses $N$ independent hash tables, each containing a
prime number of counters $M$.  The hashing function for each hash
table is fixed, and bijectively converts each DNA k-mer (for k $<=$ 32)
into a 64-bit number to which the modulus of the hash table size is
applied.  This provides $N$ distinct hash functions (see also
\cite{adina2013}).

To increment the count associated with a k-mer, the counter associated
with the hashed k-mer in each of the $N$ hash tables is incremented.
To retrieve the count associated with a k-mer, the minimum count
across all $N$ hash tables is chosen.

In this scheme, collisions are explicitly not handled, so the count
associated with a k-mer may not be accurate. Because collisions only
falsely {\em increment} counts, however, the retrieved count for any
given k-mer is guaranteed to be no less than the correct count.  Thus
the counting error is one-sided.

\subsection{Source code and scripts}

We implemented this approach to k-mer counting in a software package
named khmer, written in C++ with a Python wrapper.  khmer is freely
available under the BSD license at http://github.com/ged-lab/khmer/.

The version of khmer used to generate the results below is available
at 
bleeding-edge  http://github.com/ged-lab/khmer.git.(retrieved on 2013-05-31)  Scripts specific to this paper are available in the paper
repository at https://github.com/ged-lab/2013-khmer-counting.






\bibliography{khmer-counting}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
   & size of file(GB) & number of reads & number of unique
  k-mers & total number of k-mers \\
  \hline
dataset1 & 1.90 & 9,744,399 & 561,178,082 & 630,207,985
\\
dataset2 & 2.17 & 19,488,798 & 1,060,354,144 & 1,259,079,821
\\ 
dataset3 & 3.14 & 29,233,197 & 1,445,923,389 & 1,771,614,378
\\ 
dataset4 & 4.05 & 38,977,596 & 1,770,589,216 & 2,227,756,662
\\ 
dataset5 & 5.00 & 48,721,995 & 2,121,474,237 & 2,743,130,683
\\
  \hline  
\end{tabular}

\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
  ~ÊÊÊÊÊÊÊ & \bigcell{c}{Real metagenomics\\reads} & \bigcell{c}{Totally random 
reads\\with randomly\\ generated k-mers} & \bigcell{c}{Simulated reads\\from 
simulated\\genome with error} & \bigcell{c}{Simulated reads\\from 
simulated\\genome without error}  \\
  \hline
ÊÊÊÊÊÊÊÊSize of data set file      & 7.01MÊÊÊ & 3.53MÊÊÊÊ& 5.92MÊÊ   & 9.07M 
ÊÊÊÊÊÊÊÊÊÊÊÊ \\ 
ÊÊÊÊÊÊÊÊNumber of total k-mers     & 2917200Ê & 2250006ÊÊ& 3757479ÊÊÊ& 
5714973ÊÊÊÊÊÊÊÊÊ \\ 
ÊÊÊÊÊÊÊÊNumber of unique k-mers    & 1944996  & 1973430ÊÊ& 1982403ÊÊ & 
1991148ÊÊÊÊÊÊÊÊÊÊ \\ 
  \hline  
\end{tabular}

%\graphicspath{./figure/}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/time_benchmark}}
\caption{Time usage of different khmer counting tools}
\label{cmp_time}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/memory_benchmark}}
\caption{Memory usage of different k-mer counting tools}
\label{cmp_memory}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/disk_benchmark}}
\caption{Disk storage usage of different k-mer counting tools}
\label{cmp_disk}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset_vs_fpr}}
\caption{relation between average miscount and counting error rate, note x axis starts at 0.3}
\label{average_offset_vs_fpr}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_unique_pos}}
\caption{Percentage of the unique k-mers starting in different position in 
reads}
\label{perc_unique_pos}
\end{figure}



\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_remaining_reads}}
\caption{Number of remaining reads after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_remaining_reads}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_bad_reads.pdf}}
\caption{Number of reads with low-abundance k-mers after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_bad_reads}
\end{figure}
