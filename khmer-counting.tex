\documentclass{article}
\usepackage{simplemargins}

%\usepackage{multirow}
\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}
\bibliographystyle{plain}
\setlength{\parindent}{0pt} \setlength{\parskip}{1.6ex}
\setallmargins{1in} \linespread{1.6}

% @CTB do we want to talk about k-mer frequency spectrum instead of k-mer
%      counting?
% @CTB mention altmet stuff? ``top X packages''
% @CTB be sure to mention Conway & Bromage paper.
% @CTB point out that implementation is really easy!
% @CTB discuss importance of in-memory counting
% @CTB relate to compression, too.
% @CTB discuss tunable error rate.

% @CTB mention altmetrics/popularity; API
% @CTB put in 'diff' command in Makefile
% @CTB abundance hist computation/comparison => text
% @CTB for figure 1 (time) do we want x axis to be reads or k-mers? would argue
%    reads, not k-mers.

% @CTB new stuff/TODO for submission:
% @CTB   put in full author names, etc
% @CTB   spellcheck
% @CTB   put in github tag references
% @CTB   ref the use of Amazon for benchmarks; cloud environment.
% @CTB   mention: can update at any time!
% @CTB   add in k-mer search
% @CTB   adjust to rayan's language
% @CTB   do we add in .mct building into benchmarks?

\begin{document}



% Title must be 150 characters or less
\begin{flushleft}
{\Large \textbf{A Probabilistic Approach to k-mer Counting} }
% Insert Author names, affiliations and corresponding author email.
\\
Qingpeng Zhang$^{1}$, 
Eric McDonald$^{1}$,
Jason Pell$^{1}$,
Rosangela Canino-Koning$^{1}$,
Adina Chuang Howe$^{2,3}$,and 
C. Titus Brown$^{1,2\ast}$
\\
\bf{1} Computer Science and Engineering, Michigan State University, East Lansing, MI, USA
\\
\bf{2} Microbiology and Molecular Genetics, Michigan State University, East Lansing, MI, USA
\\
\bf{3} Plant, Soil, and Microbial Sciences, Michigan State University, East Lansing, MI, USA
\\
$\ast$ E-mail: ctb@msu.edu
\end{flushleft}

\section{Abstract}

\paragraph{Introduction:}

K-mer abundance analysis is widely used for many purposes in sequence
analysis, including data preprocessing for de novo assembly, repeat
detection, and sequencing coverage estimation.  Recently, a number of
new k-mer counting libraries have emerged to handle the increasing
amount of data available from sequencing platforms; these libraries
offer various trade-offs between memory and disk usage.

\paragraph{Results:}

We present the khmer software package for fast and memory efficient
counting of individual k-mer abundances in sequencing data
sets. Unlike previous methods based on data structures such as hash
tables, suffix arrays, and trie structures, khmer relies entirely on a
simple probabilistic data structure, a Count-Min Sketch.  On sparse
data sets, this data structure is considerably more memory efficient
than any exact data structure.  The trade-off for this memory
efficiency is that the use of a Count-Min Sketch introduces a
systematic positive overcount for k-mers.  Here we analyze the
analysis speed, memory usage, and miscount rate of our Count-Min
Sketch implementation for generating the k-mer frequency distribution
in simulated and real sequencing data sets.  We also compare the
performance of khmer to several other k-mer counting packages,
Tallymer, Jellyfish, and DSK.  Finally, we present two applications of
khmer on short-read sequencing data: first, doing reference-free error
analysis of short-read data with k-mer abundances; and second,
trimming errors from short reads.

\paragraph{Conclusion:}

The Count-Min Sketch, as implemented in khmer, is an effective and
efficient tool for k-mer counting in biological sequences.  In
particular, the miscount behavior of the Count-Min Sketch performs
well on error-prone short-read sequencing data.  We show that the
khmer package offers an efficient set of trade-offs for certain k-mer
counting applications.  khmer is implemented in C++ wrapped with a
Python interface, and is freely available under the BSD license at
github.com/ged-lab/khmer.

\section{Introduction}

A k-mer is a substring of length k in a DNA sequence, where k is
usually between 4 and the length of a short sequencing read. The goal
of k-mer counting is to determine the number of occurrences for each
k-mer in a dataset composed of many sequence reads. Efficient k-mer
counting plays an important role in many bioinformatics approaches,
including data preprocessing for de novo assembly, repeat detection,
and sequencing coverage estimation

One particular application of k-mer frequency analysis is the detection
and removal of sequencing errors.  Because sequencing errors generate
many erroneous k-mers with low abundance, we can optimize for more
heavyweight computational approaches such as assembly by removing
reads with too many low-abundance k-mers prior to assembly. Similarly,
we can estimate coverage and remove redundancy by analyzing k-mer
distributions within reads \cite{Brown2012}.  In both cases,
pre-assembly filtering of reads to reduce dataset sizes is a crucial
component of time and memory reduction. Additionally, we can use k-mer
counting to evaluate genome size and the coverage of sequencing reads,
which can help determine parameter settings and analyze assembly
results \cite{Chikhi2013}. K-mer counting can also give important information for
predicting regions with repetitive elements such as
transposons \cite{Kurtz2008}.

The central challenge for k-mer counting in short-read shotgun
sequencing data set is that these data sets are both relatively sparse
in k-mers
and contain many erroneous k-mers.  The data sets are sparse because for
typical values of k such as k=32, only a small fraction of the total
possible number of k-mers ($4^{32}$) are actually present in the data
set.  The high error rate (Illumina has a ~0.1-1\% per-base error rate) generates many unique k-mers.  In particular,
as the total number of
reads generated increases, the total number of errors grows linearly,
leading to data sets where the erroneous k-mers vastly outnumber the
true k-mers \cite{Conway2011}.  Dealing with the resulting large numbers of k-mers, most of which
are erroneous, has become an unavoidable and challenging task
\cite{Minoche2011}.

A variety of k-mer counting approaches, and standalone software
packages implementing them, have emerged in recent years; this
includes Tallymer\cite{Kurtz2008}, Jellyfish\cite{Marcais2011}, BF-Counter\cite{Melsted2011}, DSK\cite{Rizk2013}, KMC\cite{Deorowicz2013}, and Turtle\cite{Roy2013}.
% @CTB see http://arxiv.org/abs/1305.1861)
% @CTB include more discussion of BFcounter & turtle?
These
approaches and implementations each offer different algorithmic
trade-offs and different efficiencies, and enable a non-overlapping set
of functionality.  Tallymer uses a suffix tree to store k-mer counts
in memory and on disk.  Jellyfish stores k-mer counts in in-memory
hash tables, and makes use of disk storage to scale to larger
data sets.  BF-Counter uses a Bloom filter as a pre-filter to avoid
counting unique k-mers, and is the first published probabilistic approach
to k-mer counting.  DSK adopts a streaming approach to k-mers that
enables time- and memory-efficient k-mer counting with an explicit
trade-off between disk and memory usage.  And KMC relies primarily
on fast and inexpensive disk access to count k-mers in very little
memory.

Our motivation for exploring efficient k-mer counting comes from our
work with metagenomic data, where we routinely encounter data sets
that contain 300e12 bases of DNA and over 50 billion distinct k-mers
\cite{Howe2012}.  In order to efficiently filter, partition, and
assemble these data, we need to store counts for each of these k-mers
in main memory.  This has dictated our exploration of efficient
in-memory k-mer counting techniques.

Below, we describe an implementation of a simple probabilistic data
structure for k-mer counting.  This data structure is based on a
Count-Min Sketch, a generalized probabilistic data structure for
storing the frequency distributions of distinct elements
\cite{Cormode2005}.  Our implementation is based on an extension of
a Bloom filter, which has been previously used for k-mer counting
and de Bruijn graph storage and traversal
\cite{Bloom70} \cite{BroderM03} \cite{Melsted2011} \cite{Pell2012}.


The CountMin Sketch approach is particularly memory efficient, with
memory usage that can significantly outperform exact data structures
 \cite{Pell2012}\cite{Conway2011}.  However, the use of a probabilistic
data structure introduces counting errors, which have effects that
must be analyzed in the context of specific problems.  Below, we
compare CPU, memory and disk usage of our implementation with that of
Tallymer, Jellyfish, and DSK, and show that khmer is competitive in speed,
memory, and disk usage.  We also analyze the effects of this
counting error on calculations of the frequency distribution for
sequencing data sets, and in particular on metagenomic data sets.

\section{Results}

\subsection{Implementing a CountMin Sketch for k-mers}

The implementation details are similar to those of the Bloom filter in
\cite{Pell2012}, but with the use of 8 bit counters instead of 1 bit
counters.  Briefly, Z hash tables are allocated, each with a different
size of approximately H bytes; the sum of these hash table sizes must
fit within available main memory.  To increment the count for a
particular k-mer, a single hash is computed for the k-mer, and the
modulus of that hash with each hash table's size H gives the location
for each hash table; the associated count in each hash table is then
incremented by 1.  To retrieve the count for a k-mer, the same hash is
computed and the minimum count across all hash tables is computed.
While different in implementation detail from the standard CountMin Sketch,
which uses a single hash table with many hash
functions, the performance details are identical \cite{Pell2012}.

To analyze the false positive rate -- the probability with which a
given k-mer count will be incorrect -- we can look at the hash table
load. Suppose N unique k-mers have been counted using Z hash tables,
each with size H.  The probability that no collisions happened in a
specific entry in one hash table is $(1-1/H)^{N}$, which can be
approximated as $e^{-N/H}$. The individual collision rate in one hash
table is $1-e^{-N/H}$. The total collision rate, which is the
probability that a collision occurred in each entry where a k-mer maps
to in all Z hash tables, is $(1-e^{-N/H})^{Z}$. In this situation, the
counts in all Z hash table bins cannot give the true count of a k-mer.

While the false positive rate can easily be calculated from the hash
table load, the average {\em miscount} depends on the k-mer frequency
distribution, which must be determined empirically.  We analyze this below.

\subsection{khmer can count k-mers efficiently}

We measured the time and memory usage to calculate k-mer abundance
histograms in five soil metagenomic read data sets using khmer,
Tallymer, Jellyfish, and DSK (Figure \ref{cmp_time} and Figure
\ref{cmp_memory}).  We chose to benchmark abundance histograms because
this was a built-in feature common to all four software packages.

Figure \ref{cmp_time} shows that the time usage of our khmer approach
is comparable to Tallymer, but is slower than Jellyfish. From Figure
\ref{cmp_memory}, we see that the memory usage of both Jellyfish and
Tallymer increases linearly with dataset size, although Jellyfish is
more efficient than Tallymer in memory usage for smaller k size. Using
option -parts 4 for Tallymer subroutine suffix reduces the memory
usage. But the second step of the Tallymer counting method - the
mkindex subroutine -- will always use more memory as the number of k-mers
being counted increases.  For a 5 GB dataset with 2.7 billion total k-mers,
Jellyfish uses 5 GB memory; Tallymer exceeds 24 GB of memory usage for
a smaller 4 GB data set.

In addition, the memory usage of khmer also increases linearly with
data set size as long as we hold the error rate constant.  However,
the memory usage of khmer varies substantially with the desired false
positive rate: we can decrease the memory usage by increasing the
false positive rate, as shown in Figure \ref{cmp_memory}.  We can also
see from the figure that with a low false positive rate of 1\%, the
memory usage is competitive with other programs; with a higher 5\%
false positive rate, the memory usage is better than all but the
disk-based DSK.

Another concern is disk usage: both Jellyfish and Tallymer use large
index files stored on the hard disk.  Figure \ref{cmp_disk} shows that
the disk usage also increases linearly with the dataset size. For a
dataset of 5 GB, the disk usage of both Jellyfish and Tallymer is
around 30 GB.  khmer does not rely on any on-disk storage, although
for practicality the hash tables can be saved for later reuse; thus
the uncompressed disk usage for khmer in Figure \ref{cmp_disk} is the
same as its memory.

DSK performs extremely well in memory, speed and disk space for
calculating the abundance distribution of k-mers. However, in exchange
for this efficiency, retrieving specific k-mer counts at random is
likely to be quite slow, as DSK is optimized for iterating across
partition sets of k-mers rather than retrieving individual k-mer
counts.  For this reason, DSK is not a general solution to k-mer
counting.  Since neither Tallymer nor Jellyfish have a programmatic
API for retrieving specific k-mer counts, we could not compare
performance for retrieving the counts of specific k-mers.

% @CTB ref UW QUIP?
% @CTB is this the benchmark we should be doing? probably. sigh.
% see last para :)

\subsection{The measured counting error rate is low on short-read data}

A large number of the k-mers in short-read
dataset are unique \cite{Melsted2011}. Here we use both real and simulated
datasets to evaluate the counting performance in practice.  Our
primary measure for evaluation is the {\em offset}, which is the
difference between the true k-mer frequency and the frequency reported
by khmer.

Figure \ref{average_offset_vs_fpr} shows the relationship between average 
miscount and counting error rate for different test data sets.

Even when the counting error rate is as high as 0.9, which means that
90\% of k-mers will have an incorrect count, the average offset can
still be kept pretty low.

% @CTB what is the coverage of the simulated data sets?
% @QP coverage=3


\subsection{Sequencing error profiles can be measured with k-mer abundance
profiles in reads}

One specific use for khmer is detecting random sequencing errors by
looking at the k-mer spectrum within reads \cite{Medvedev2011}
. Low-abundance k-mers contained in a high-coverage data set
typically represent random sequencing errors, and a variety of read
trimming and error correcting tools use k-mer counting to reduce the
error content of the read data set, independent of quality scores or
reference genomes \cite{Kelley2010}.  This is an application where the counting error
effect of the CountMin Sketch approach used by khmer may be particularly tolerable,
because khmer never underestimates counts, so will never falsely call an erroneous k-mer.

Here we look at the sequencing error pattern of an E. coli Illumina
reads dataset as an example.  

%(@CTB which data set? And I'm going to
%assume it's not trimmed in any way, right? Just all reads? And what's
%the estimated coverage?)
%@QP:     ecoli_ref.fastq  (read length: 100bp) It is not trimmed.
%@    got from diginorm paper dataset: /scratch/titus/diginorm/ecoli_ref.fastq
%@ from diginorm paper:
%@ The E. coli,... data sets were taken from Chitsaz et al. [15], and downloaded from bix.ucsd.edu/projects/singlecell/.


As shown in Figure \ref{perc_unique_pos} there are more unique k-mers
close to the 3' end of reads.  This is almost certainly due to the
increased error rate at the 3' end of reads. For metagenomic data sets
with variable coverage, this approach is merely diagnostic for the
entire data set and cannot necessarily be used to trim reads, because
there may be many low abundance k-mers; however, this is useful
information for deciding at what position in a sequencing run the error rates become unacceptably high and
reads should be trimmed.

\subsection{Using khmer to trim errors from reads}

% @CTB make sure we make the effects of false positives clear.

As discussed above, we can detect erroneous k-mers in high coverage
data sets by finding low abundance k-mers.  Removing or trimming reads
containing unique or low-abundance k-mers will remove many errors,
which can help scale de Bruijn graph based assembly methods.
\cite{Qin2010} \cite{Hess2011} \cite{Mackelprang2011}
% all of which used low abundance trimming for metagenomic data).

The khmer k-mer counting approach can filter reads based on
k-mer abundance, and can be used for arbitrary k. One approach to
k-mer abundance filtering involves removing any read that contains
even a single low-abundance k-mer.
This filtering can be implemented
in two passes: the first pass for counting k-mers from reads and the
second pass for filtering the reads. The counting error here manifests
as an overestimated count in hash entries with one or more collisions,
in which case k-mers hashing to that entry may not be correctly
flagged as low-abundance.  High counting error rate therefore manifest
as "lenient" filtering, in which reads may not be properly
removed. However, any read that is trimmed will be correctly
trimmed. To reduce the effect of such counting error rate, we can do
the filtering iteratively; after each run of filtering, more
reads with low-abundance k-mers will be discarded.

As an example of this method, we filtered out reads with low abundance
k-mers from a human gut microbiome metagenomic dataset (MH0001) with
more than 42 million reads. In fact we want any read with any unique
k-mer to be discarded.  We used hash tables with different size to
show the influence of hash table size. We also showed the effect of
iterative filtering to reduce false positive rate. To assess the
counting error rate, we used Tallymer to obtain the actual accurate
count of the k-mers in the dataset.

From Figure \ref{num_remaining_reads}, we see that after each run,
more low-abundance reads were discarded. With larger hash table, the
low-abundance reads were discarded faster. On the other hand, from
Figure \ref{num_remaining_reads}, we can see that after each
iteration of filtering, the percentage of ``bad'' reads - reads with
unique k-mers - decreased. After four iterations, the percentage of
bad reads was less than 4\%. The result showed that with our method
nearly 40\% of the original reads were discarded by removing the
low-abundance reads with an acceptable false positive rate (less than
4\% after four iterations of filtering). This reduces the memory and
time requirements considerably in follow-on assembly \cite{Howe2012}.

% @CTB what is the initial false positive rate here? It'd be nice
% to show how iterative filtering can lead to substantial improvements
% in the second round, if enough k-mers are removed in the first round.
% Also, can we use ``number of reads with low-abundance k-mers'' instead
% of number of remaining reads on the y axis?
% @QP Figure 7 But here one of the purpose of figure 6 is that it can show the
% iteration of filtering can reduce the number of remaining reads by discarding
% reads with low-abundance k-mers. Figure 7 can not show this information.

% @CTB do we want to mention diginorm especially?

\section{Discussion}

\subsection{khmer memory usage is fixed and low}

The memory usage of the basic CountMin Sketch approach is fixed, which means
that khmer will never crash due to memory limitations, and all
operations can be performed in main memory without recourse to disk
storage.  However, the memory size chosen must be considered in light
of the false positive rate and miscount acceptable for a given
application.  In practice, we have found that a false positive rate of
between 1\% and 10\% offers acceptable miscount performance for a wide range of
tasks, including digital normalization and low-abundance read-trimming.

The memory usage of khmer is also quite low for sparse data sets,
especially since only main memory is used and no disk space is
necessary beyond that required for the read data sets.  This is no
surprise: the information theoretic comparison shown in
\cite{Pell2012} shows that, for sparse sequencing data sets, Bloom
filters require considerably less memory than any possible exact
information storage for a wide range of error rates and data set
sparseness.

As discussed above, for a given dataset, the size and number of hash
tables will determine the accuracy of k-mer counting. Thus, the user
can control the memory usage based on the desired level of
accuracy. The time usage for the first step of k-mer counting , to
consume the k-mers into a counting data structure, depends on the
total amount of data since we must walk through every k-mer in each
read. The second step, k-mer retrieval, is essentially constant for
fixed k and Z.

In our implementation, to reduce the memory usage as much as possible,
we use 1 byte to store the count of each k-mer in the data
structure. This means that the maximum count for a k-mer will be 255.
In cases where tracking bigger counts is required, khmer also provides an
option to use an STL map data structure to store counts above 255,
with the trade-off of significantly higher memory usage.  In the
future, we may extend khmer to counters of arbitrary bit sizes.

\subsection{khmer scales k-mer counting significantly and usefully}

From the performance comparison between khmer and two other k-mer
counting packages, we can conclude that for large metagenomic
datasets, it becomes impractical to count k-mers using Jellyfish and
Tallymer because of the high memory and disk usage.  (What about DSK?
@CTB) However, the khmer counting approach can do the counting in
lower memory and disk usage, although with the trade-off of
inaccuracy. For sparse data sets like metagenomic sequencing reads,
this memory usage is better than any possible exact counting scheme,
\cite{Pell2012}.  More generally, we show above that for sequencing
data with many low-abundance k-mers, probabilistic approaches that
make use of the 1-biased frequency distribution can be used quite
efficiently.
% @CTB revise the above paragraph

As shown in Figure \ref{cmp_memory}, khmer's memory usage is
independent of k, while the memory usage of Jellyfish depends on
heavily on the length of k-mers.  In addition, for smaller k-mer sizes
and/or less sparse data sets, we would expect suffix tree approaches
such as those used in Tallymer to outperform khmer and Jellyfish.

\subsection{Error rates in k-mer counting are low and predictable}

The Count-Min sketch is a probabilistic data structure and the
counting is not exactly correct. This generates a one-sided error that
can result in an overestimate of k-mer frequency, but cannot generate
an underestimate. The chosen parameters of the data structure will
influence the accuracy of the count.  While the probability of an
inaccurate count can easily be estimated based on the hash table load,
the miscount size is dependent on details of
the frequency distribution of k-mers \cite{Cormode2005}.

More specifically, in the analysis of the Count-Min
sketch\cite{Cormode2005}, the offset between the incorrect count and
actual count is related to the total number of k-mers in a dataset and
the size of each hash table. Further study has shown that the behavior of
Count-Min sketch depends on the characteristics of data set such as
skewness\cite{Rusu2008} \cite{CormodeM05}.

Such probabilistic properties suit short reads from next
generation sequencing data sets; that is, the counts
are not so wrong for next generation sequencing reads data sets
because of the highly skewed abundance distribution of k-mers in those
data sets.

% @CTB maybe move this to results, or split it between there and here.
Figure \ref{average_offset_vs_fpr} demonstrates these properties very well. It
shows the relationship between average miscount and counting error
rate for different test data sets. For a fixed counting error rate,
the simulated reads without error have the highest average miscount and
the simulated k-mers data have the lowest average miscount.  This is because
they have the highest and lowest number of total k-mers,
respectively. We can have more correct counting for real error-prone
reads from a genome than for randomly generated reads from a genome
without error and with a normal distribution of k-mer abundance. Thus,
we can conclude that our counting approach is especially suitable for high
diversity data sets, like simulated k-mers and real metagenomics data,
in which a larger proportion of k-mers are low abundance or unique due
to sequencing errors.

\subsection{khmer applications}

For many applications, an approximate k-mer count is sufficient.
For example, when we want to eliminate reads with low
abundance k-mers, we can tolerate the fact that a certain number of
reads with low frequency will remain in the resulting data set falsely
because of the frequency inflation caused by collision. If necessary
we can do the filtering iteratively so that at each step, reads with low
abundance k-mers can be discarded, decreasing the false positive rate for the
next step. Another
example is measuring sequencing error profiles within reads using k-mer abundance
profiles. As shown in above section, we can
use the abundance distribution of unique k-mers, since the counting
errors from collision will not influence the count of these unique
k-mers. Furthermore, because the rate of inaccurate counting can be
predicted, we can adjust the parameters of the data
structure to make sure the count accuracy satisfies our downstream
analysis.

This k-mer counting structure is also highly scalable. For
certain sequence data sets, counting error rate is related to memory
usage. Generally, the more memory we can use, the more accurate the
counting will be. However, no matter how large the data set is, we can
predict and control the memory usage well with choosing specific
parameters. Given certain parameters like the size and number of hash
tables to use, the memory usage is constant and independent of the
length of k-mer and the size of the sequence data set to count. Our
method will never break an imposed memory bound, unlike some other
methods.

@CTB This graceful
degradation in the face of large amounts of data is a key property of
this k-mer counting approach.

@CTB discuss API

@CTB mention again why we disqualify DSK

\subsection{Future}

Scaling the Bloom counting hash to extremely large data sets with many
unique k-mers requires a large amount of memory: approximately 446 GB
of memory is required to achieve a false positive rate of 1\% for $N \approx
50x10^9$. It is possible to reduce the required memory by dividing
k-mer space into multiple partitions and counting k-mers separately
for each partition. Partitioning k-mer space into $N$ partitions
results in a linear decrease in the number of k-mers under
consideration, thus reducing the occupancy by a constant factor $n$
and correspondingly reducing the collision rate.  This is similar to
the approach taken by DSK \cite{Rizk2013}.

% @CTB combine
Partitioning k-mer space is a generalization of the systematic
prefix filtering approach, where one might first count all k-mers
starting with AA, then AC, then AG, AT, CA, etc., which is equivalent
to partitioning k-mer space into 16 equal-sized partitions. These
partitions can be calculated independently, either across multiple
machines or iteratively on a single machine, and the results stored
for later comparison or analysis.

% @CTB fix
Multi-core architectures are popular nowadays. It is 
promising to try to take advantage of them using parallelization.
However, because k-mer 
counting problem relies on high throughput processing of data, I/O bandwidth 
will be essential constraint beyond a certain point of parallelization.
That said, we still implemented the parallelization in some degree. We
have implemented a pre-fetch buffer in conjunction with direct input that
can be used by multiple threads. Such parallel computing makes the good
 scaling of the khmer software possible.

In the future, after the basic performance issues is solved, we will be
focused on the growing of the programmer's API. Currently Khmer has been
widely used by many groups because of the versatility of this package. 
It can not only do the routine k-mer counting or k-mer abundance distribution
analysis as other k-mer counting tools, but also has a user-friendly API 
which can be used to do more fancy job and many useful scripts working right 
out of the box. Next We will still work on providing more well-characterized
components that can be integrated into larger pipelines and providing more
well tested use cases and documentation to make it more accessible to people
lack of computing experience.



\section{Conclusions}
% 1. Fairly general solution given the error rates.
% 2. Scales a wide range of applications.
% 3. Tunable mem usage/fp rate.
In summary this is a generally useful approach. It can be widely used to
deal with next generation sequencing short reads data sets effectively 
and efficiently.
K-mer counting has been widely used in many bioinformatics problems,
including data preprocessing for de novo assembly, repeat detection,
sequencing coverage estimation. Here we present the khmer software
package for fast and memory efficient counting of k-mers. Unlike
previous methods bases on data structures like hash tables, suffix
arrays, or trie structures, Khmer uses a simple probabilistic data
structure, which is similar in concept to Count-Min sketch. It is
highly scalable, effective and efficient in analyzing large next
generation sequencing dataset involving k-mer counting, despite with
certain counting error rate as tradeoff. We compared the memory usage,
disk usage and time usage between our khmer program and other programs
like Tallymer and Jellyfish to show the advantage of our method. The
counting accuracy was also assessed theoretically and was validated
using simulated data sets. Our k-mer counting approach can be used
efficiently and effectively for any high diverse dataset with lots of
low-abundance k-mers, like next generation sequencing data sets, which
are biased towards low-abundance k-mers due to errors. We further
showed applications of khmer software package in tackling problems
like detecting sequencing errors in metagenomic reads and removing
those erroneous reads to reduce data set size through efficient k-mer
counting. Our approach can also be implemented parallelly or
distributably to speed up or handle larger data sets with reasonable
counting error rate.

\section{Methods}

\subsection{Sequence Data}

Two human gut metagenome reads datasets (MH0001 and MH0002) were used from the 
MetaHIT (Metagenomics of the Human Intestinal Tract) project\cite{Qin2010}. 
The MH0001 dataset contains approximately 59 million reads, each 44bp long. 
The MH0002 dataset consists of about 61 million 75bp long reads.
We trimmed each FASTA file to remove low quality sequences. 
% @CTB how did we trim?  What script, exactly?
% @ Adina did this for me. I will ask her about this. -QP

Five soil metagenomics reads data sets with different size were taken
from GPGC project for benchmark purpose.
(Iowa Prairie Table 1\ to cite here.)
% @CTB we will need accession numbers.  Adina has them.

We also generated four short read data sets to assess the false
positive rate and miscount distribution. One is a subset of a real
metagenomics data set from the MH0001 dataset, above. The second consists of randomly generated reads. The third
and fourth contain reads simulated from a random, 1 Mbp long genome.
The third has a substitution error rate of 3\%, and the fourth contains
no errors. The four data sets were chosen to contain identical numbers
of unique 22-mers.

\subsection{Hash function and khmer implementation}

The current khmer hash function works only for $k <= 32$ and converts
DNA strings exactly into 64-bit numbers. Each of the $Z$ hash table
sizes $H_i$ is chosen to be a different prime number larger than the
user-configured hash table size $H$, and then these $H_i$ are used to
generate table-specific hash values with the modulus function.
However, any hash function would work, e.g. a cyclic hash, which would
enable khmer to count k-mers larger in size than 32; this would not
change the scaling behavior of khmer at all, and is a planned
extension.

By default khmer counts k-mers in DNA sequence, i.e. strandedness is
disregarded by having the hash function choose the lower numerical
value for the exact hash of both a k-mer and its reverse complement.
This behavior is configurable via a compile-time option.

\subsection{Comparing with other k-mer counting programs}

We generated k-mer abundance histograms from five soil metagenomic reads
datasets of different sizes using our khmer counting package and
three other k-mer counting packages - Tallymer, Jellyfish and DSK -  to compare
performance.  We fixed k at 22.

For khmer, we set hash table sizes to fix the false positive rate at
either 1\% or 5\%, and used 8 threads in loading the data.

Tallymer is from the genometools package version 1.3.4, and was run
with the following options. For the suffixerator subroutine we used:
{\tt -dna -pl -tis -suf -lcp}.  We varied the {\tt -parts n} option to
create the index with only 1/n of the total data in main memory, which
reduces the memory usage.  We separately used {\tt -parts 4} and {\tt
  -parts 1} to test performance.

For the mkindex subroutine, we used: {\tt -mersize 31} and {\tt -mersize 22}.

Jellyfish is version 1.1.2 and the multithreading option is set to 8 threads.

Jellyfish uses a hash table to store the k-mers and the size of the
hash table can be modified by the user.  When the specified hash table
size is not large enough and fills up, jellyfish writes it to the hard
disk and initializes a new hash table to more k-mers.  Here we use a
similar strategy as in \cite{Melsted2011} and chose the minimum size of the hash 
tables for Jellyfish so that all k-mers were stored in memory.

We ran Jellyfish with the options as below:

{\tt jellyfish count -m 22 -c 2 -C} for k=22.

{\tt jellyfish count -m 31 -c 2 -C} for k=31.

We ran DSK with default parameters.

\subsection{Count-Min Sketch implementation}

We implemented the Count-Min Sketch data structure, a simple
probabilistic data structure for counting distinct elements \cite{Cormode2005}.  Our
implementation uses $N$ independent hash tables, each containing a
prime number of counters $M$.  The hashing function for each hash
table is fixed, and bijectively converts each DNA k-mer (for k $<=$ 32)
into a 64-bit number to which the modulus of the hash table size is
applied.  This provides $N$ distinct hash functions (see also
\cite{adina2013}).

To increment the count associated with a k-mer, the counter associated
with the hashed k-mer in each of the $N$ hash tables is incremented.
To retrieve the count associated with a k-mer, the minimum count
across all $N$ hash tables is chosen.

In this scheme, collisions are explicitly not handled, so the count
associated with a k-mer may not be accurate. Because collisions only
falsely {\em increment} counts, however, the retrieved count for any
given k-mer is guaranteed to be no less than the correct count.  Thus
the counting error is one-sided.

\subsection{Source code and scripts}

We implemented this approach to k-mer counting in a software package
named khmer, written in C++ with a Python wrapper.  khmer is freely
available under the BSD license at http://github.com/ged-lab/khmer/.

The version of khmer used to generate the results below is available
at 
bleeding-edge  http://github.com/ged-lab/khmer.git.(retrieved on 2013-05-31)  Scripts specific to this paper are available in the paper
repository at https://github.com/ged-lab/2013-khmer-counting.






\bibliography{khmer-counting}

\begin{table}[ht]
    \begin{tabular}{ |c | c |c| c|c| }
      \hline                        
       & size of file(GB) & number of reads & number of distinct
      k-mers & total number of k-mers \\
      \hline
    dataset1 & 1.90 & 9,744,399 & 561,178,082 & 630,207,985
    \\
    dataset2 & 2.17 & 19,488,798 & 1,060,354,144 & 1,259,079,821
    \\ 
    dataset3 & 3.14 & 29,233,197 & 1,445,923,389 & 1,771,614,378
    \\ 
    dataset4 & 4.05 & 38,977,596 & 1,770,589,216 & 2,227,756,662
    \\ 
    dataset5 & 5.00 & 48,721,995 & 2,121,474,237 & 2,743,130,683
    \\
      \hline  
    \end{tabular}
\end{table}

\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\begin{tabular}{ |c | c |c| c|c| }
  \hline                        
  ~ÊÊÊÊÊÊÊ & \bigcell{c}{Real metagenomics\\reads} & \bigcell{c}{Totally random 
reads\\with randomly\\ generated k-mers} & \bigcell{c}{Simulated reads\\from 
simulated\\genome with error} & \bigcell{c}{Simulated reads\\from 
simulated\\genome without error}  \\
  \hline
ÊÊÊÊÊÊÊÊSize of data set file      & 7.01MÊÊÊ & 3.53MÊÊÊÊ& 5.92MÊÊ   & 9.07M 
ÊÊÊÊÊÊÊÊÊÊÊÊ \\ 
ÊÊÊÊÊÊÊÊNumber of total k-mers     & 2917200Ê & 2250006ÊÊ& 3757479ÊÊÊ& 
5714973ÊÊÊÊÊÊÊÊÊ \\ 
ÊÊÊÊÊÊÊÊNumber of unique k-mers    & 1944996  & 1973430ÊÊ& 1982403ÊÊ & 
1991148ÊÊÊÊÊÊÊÊÊÊ \\ 
  \hline  
\end{tabular}

%\graphicspath{./figure/}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/time_benchmark}}
\caption{Time usage of different khmer counting tools. @CTB :reads?}
\label{cmp_time}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/memory_benchmark}}
\caption{Memory usage of different k-mer counting tools. @CTB x axis: k-mers?}
\label{cmp_memory}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/disk_benchmark}}
\caption{Disk storage usage of different k-mer counting tools}
\label{cmp_disk}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/average_offset_vs_fpr}}
\caption{relation between average miscount and counting error rate. Note, x axis starts at 0.3.  @CTB fig 4b percent?}
\label{average_offset_vs_fpr}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/perc_unique_pos}}
\caption{Percentage of the unique k-mers starting in different position in 
reads}
\label{perc_unique_pos}
\end{figure}



\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_remaining_reads}}
\caption{Number of remaining reads after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads)}
\label{num_remaining_reads}
\end{figure}

\begin{figure}
\center{\includegraphics[width=5in]{./figure/num_bad_reads.pdf}}
\caption{Number of reads with low-abundance k-mers after iterating filtering out low-abundance 
reads that contain even a single unique k-mer with hash tables with different 
sizes(1e8 and 1e9) for a human gut microbiome metagenomic dataset(MH0001, 
42,458,402 reads).  What is FP rate for hashtable size?}
\label{num_bad_reads}
\end{figure}

\end{document}
